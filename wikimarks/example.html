<!DOCTYPE html>
<html>
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
    <title>TREMA @ UNH - example</title>
    <link rel="stylesheet" type="text/css" href="../css/default.css" />
    <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  </head>
  <body>
    <div id="container">
      <div id="content">
        <div id="sidebar">
          <nav>
            <h1>Navigation</h1>
            <ul>
              <li><a href="../">Home</a></li>
              <li><a href="../wikimarks">Wikimarks</a></li>
            </ul>
          </nav>
        </div>

        <section id="main">
        <h1 id="wikimarks">Wikimarks</h1>
<h2 id="example-horseshoe-crab">Example: Horseshoe Crab</h2>
<p>We provide an example of the outputs of our conversion pipeline and how we create Wikimarks for several tasks.</p>
<p>The topic <code>Horseshoe</code> crab has articles on both English Wikipedias</p>
<ul>
<li>English Wikipedia <a href="https://en.wikipedia.org/wiki/Horseshoe_crab">article</a></li>
<li>Simple English Wikipedia <a href="https://simple.wikipedia.org/wiki/Horseshoe_crab">article</a></li>
</ul>
<p>We will provide outputs of these examples as generated by our conversion pipeline.</p>
<h2 id="dump-conversion">Dump Conversion</h2>
<p>The first phase will convert the raw Wikipedia dump into an easily-machine readable format.</p>
<ol type="1">
<li>download the Wikipedia and Wikidata dumps,</li>
<li>parse the Wikitext format</li>
<li>resolve redirects, disambiguation pages, and categories and expose them as metadata for each article</li>
</ol>
<p>We call the outputs of this phase “unprocessed”, which is technically not true (we came to regret the name), but it is unprocessed in the sense that all pieces of information from the original Wikipedia article are preserved.</p>
<h2 id="processing-and-filtering">Processing and Filtering</h2>
<p>The next phase will further process the “unprocessed” dump by</p>
<ol type="1">
<li>removing non-article pages,</li>
<li>removing administrative section headings (like “References”, “See also” etc)</li>
<li>removing infoboxes and images (although the pipeline can be configured to preserve those)</li>
<li>Optionally, near-duplicate paragraphs can be deduplicated – this can be important as there are many articles written by copying-pasting-modifying existing articles.</li>
<li>a corpus of all paragraphs across all Wikipedia articles is extracted</li>
</ol>
<p>We call the outputs of this phase “processed”.</p>
<p>The two example articles will look as follows</p>
<ul>
<li>English Wikipedia: <a href="http://trec-car.cs.unh.edu/wikimarks/datareleases/horseshoe-crab.article-en.json">page in JSON</a></li>
<li>Simple English Wikipedia: <a href="http://trec-car.cs.unh.edu/wikimarks/datareleases/horseshoe-crab.article-simple.json">page in JSON</a></li>
</ul>
<h2 id="subset-selection">Subset selection</h2>
<p>During the next phase, different page subsets can be selected to provide a corpus of interest. This could be a random subset of all pages to obtain a smaller collection, or a pages within a set of categories, or articles of different quality levels (such as good or vital articles) or a list of pages that were manually selected, or retrieved from Wikidata’s SPARQL endpoint.</p>
<p>See the main page for a list of <a href="wikimarks.html">example subsets</a> for which subsets are provided and instructions for how to configure the <a href="code.html">pipeline</a> to produce subsets of your choice.</p>
<p>We select just the article on horseshoe crabs via its Wikidata QID</p>
<pre><code>{ name = &quot;horseshoe-crab&quot;;
  predicate = &quot;qid-in-set [\&quot;Q1329239\&quot;]&quot;;
}</code></pre>
<p>Each subset is automatically divided into train and test splits, and each train split is provided in five folds to enable machine learning with cross-validation.</p>
<h2 id="wikimark-extraction">Wikimark extraction</h2>
        </section>
      </div>
    </div>
  </body>
</html>

