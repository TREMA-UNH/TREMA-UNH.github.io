<!DOCTYPE html>
<html>
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
    <title>TREMA @ UNH - manuscript</title>
    <link rel="stylesheet" type="text/css" href="../css/default.css" />
    <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  </head>
  <body>
    <div id="container">
      <div id="content">
        <div id="sidebar">
          <nav>
            <h1>Navigation</h1>
            <ul>
              <li><a href="../">Home</a></li>
              <li><a href="../wikimarks">Wikimarks</a></li>
            </ul>
          </nav>
        </div>

        <section id="main">
        <h1 id="wikimarks-harvesting-relevance-benchmarks-from-wikipedia">Wikimarks: Harvesting Relevance Benchmarks from Wikipedia</h1>
<p>Laura Dietz, Shubham Chatterjee, Connor Lennox, Sumanta Kashyapi, Pooja Oza, Ben Gamari</p>
<p>Manuscript under submission.</p>
<h2 id="abstract">Abstract</h2>
<p>We provide a resource for automatically harvesting relevance benchmarks from Wikipedia – which we refer to as “Wikimarks” to differentiate them from manually created benchmarks. Unlike simulated benchmarks, they are based on manual annotations of Wikipedia authors. Studies on the TREC Complex Answer Retrieval track demonstrated that leaderboards under Wikimarks and manually annotated benchmarks are very similar. Because of their availability, Wikimarks can fill an important need for Information Retrieval research.</p>
<p>We provide a meta-resource to harvest Wikimarks for several information retrieval tasks across different languages: paragraph retrieval, entity ranking, query-specific clustering, outline prediction, and query-specific entity linking and many more. In addition, we provide example Wikimarks for English, Simple English, and Japanese derived from the 01/01/2022 Wikipedia dump.</p>
<p>The resource is available at <a href="https://trema-unh.github.io/wikimarks/" class="uri">https://trema-unh.github.io/wikimarks/</a></p>
<h1 id="introduction">Introduction</h1>
<p>Many information retrieval benchmarks have complicated licensing requirements, potentially impacting the dissemination and reproducibility of approaches. Wikipedias are freely available under a Creative Commons Share-Alike license, which explicitly allows the redistribution of derived data sets.</p>
<p>With this resource paper, we provide a conversion pipeline for deriving fully automated test collections from Wikipedia for a range of information retrieval-centric tasks. Our approach is based on the assumption that Wikipedia pages were written to answer imagined information needs expressed in the page title (and/or headings). While many Wikipedia pages center on people and locations rather than general information needs, we focus on the remainder of pages which cover a wide range of topics of general interest, such as “Geomagnetic Reversal”, “Wildlife Management”, and “Superfood”.</p>
<p>Each of these page topics are interpreted within an information retrieval scenario where a user with limited prior knowledge is seeking for a general overview on the topic. A system for such information needs might retrieve, cluster, and organize information into a single integrated response. Our idea is to provide benchmarks to study each of those tasks across a shared set of topics.</p>
<p>Our approach for harvesting relevance benchmarks is based on the assumption that for information needs derived from Wikipedia titles, the actual content of the Wikipedia article constitutes a relevant response. Based on this assumption, we can train and evaluate several information-centric tasks: passage retrieval as well as query-specific subtopic clustering. Using the hyperlinks on Wikipedia articles, we can derive benchmarks for several entity-oriented tasks, such as entity retrieval, which is asking for entities that are central to the topic, as well as query-specific entity linking, which is asking to annotate text with relevant entities. Furthermore, Wikipedias are available various language versions with information about which pages correspond to the same topic across languages. This provides an opportunity for studying multi-lingual (and even cross-language) retrieval tasks.</p>
<p>We refer to our automatically harvested relevance benchmarks as “<em>Wikimarks</em>” to differentiate them from manually created benchmarks. While our prior studies <span class="citation" data-cites="dietz2020humans">[@dietz2020humans]</span> strongly suggest that evaluation results based on <em>Wikimarks</em> strongly correlate with results on manually created benchmarks, we advocate to use <em>Wikimarks</em> as a training criterion and early evaluation paradigm—to be complemented with manually created test collections.</p>
<h4 id="contributions.">Contributions.</h4>
<p>This resource provides the software and customizable toolchain for (1) converting any Wikipedia dump to an easily machine-accessible format and (2) harvesting a variety of <em>Wikimarks</em> from the dump.</p>
<p>The toolchain was recently extended to (3) provide multi-lingual support and (4) includes Wikidata<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> “Q” identifiers (QIDs) of pages, which are stable across time and language. As machine-readable interchange formats for the converted Wikipedia dump, the tool chain now supports (5) the JSON-lines<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> format (JSON-L) which allows for easy to inspection of the data model in a text editor. This is in addition to the Concise Binary Object Representation<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a> (CBOR) as defined in Internet Standard RFC 8949.</p>
<p>We provide (6) converted Wikipedia dumps from English, Simple English, and Japanese Wikipedia, dating to January 1st, 2022. As concrete examples, we (7) provide concrete <em>Wikimarks</em> for passage retrieval, entity retrieval, query-specific clustering, and entity linking for each of these three dumps.</p>
<h1 id="related-work">Related Work</h1>
<p>A wide range of manual test collections are widely used in the information retrieval community. However, the manual effort associated with their creation often prohibits the study of novel information seeking tasks, such as query-specific variants of clustering or entity linking, or outline generation for novel search result interfaces.</p>
<p>Several approaches to semi-automatic support in test collection creation are discussed, including active learning for annotation and evaluation metrics that correct for resulting biases <span class="citation" data-cites="cormack1998efficient jayasinghe2014improving yilmaz2008simple zhang2018evaluating">[@cormack1998efficient; @jayasinghe2014improving; @yilmaz2008simple; @zhang2018evaluating --inter alia]</span>. While these significantly reduce the cost, they still rely on manual annotations.</p>
<p>In this work, we provide a pipeline, dump, and methods for fully automated test collection creation. These are not intended to replace manual test collections, but to provide a means for early evaluations before significant costs are spent.</p>
<h2 id="fully-automatic-test-collections">Fully Automatic Test Collections</h2>
<p>Approaches for fully automatic benchmark creation have been discussed in the community. A popular approaches have been suggested by <span class="citation" data-cites="berendsen2013pseudo azzopardi2007building">@berendsen2013pseudo [@azzopardi2007building]</span>, where a artificial queries are simulated by selecting terms that maximize the probability of discriminating between the relevant and non-relevant document set. An open question is how to ensure that queries represent realistic information needs.</p>
<p>Alternatively metadata of articles can be exploited, such as using anchor text <span class="citation" data-cites="asadi2011pseudo">[@asadi2011pseudo]</span>, metadata of scientific articles about method, classification, and control <span class="citation" data-cites="berendsen2012generating">[@berendsen2012generating]</span>, categories in the Open Directory Project <span class="citation" data-cites="beitzel2003using">[@beitzel2003using]</span>, or glosses in Freebase <span class="citation" data-cites="dalvi2015automatic">[@dalvi2015automatic]</span>. The resource we provide in this paper is another example of this approach.</p>
<p>Within the TREC Complex Answer Retrieval track, both automatic and manual test collections were used. A study <span class="citation" data-cites="dietz2020humans">[@dietz2020humans]</span> compared the official leaderboard of submitted approaches using the manual ground truth with the leaderboard under the automatic test collection. Both leaderboards are very similar, suggesting that the automatic ground truth is useful for early method development. In this resource, we provide code for the automatic test collection and significantly extend this approach.</p>
<h2 id="related-resources-in-related-fields">Related Resources in Related Fields</h2>
<p>Several research communities are relying on Wikipedia to derive collections and benchmarks.</p>
<p>To study algorithms for fact extraction and verification, the FEVER shared task<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a> <span class="citation" data-cites="Aly21Feverous">[@Aly21Feverous]</span> uses a large collection of manually verified claims which are annotated with evidence in the form of sentences or table cells on Wikipedia pages.</p>
<p>The summarization community evaluate their algorithms on their ability to produce a summary that is similar to the lead text (i.e., text above the first section heading) given the remainder of a Wikipedia article. <span class="citation" data-cites="ghalandari2020large">@ghalandari2020large</span> derive summarization dataset from the Wikipedia event portal. Recently <span class="citation" data-cites="perez2021models">@perez2021models</span> suggest similar benchmarks for cross-lingual summarization across different language versions of the same article.</p>
<p>In open-domain aspect-based summarization, the task is to provide targeted summaries of a document from different perspectives. The WikiAsp dataset <span class="citation" data-cites="hayashi2021wikiasp">[@hayashi2021wikiasp]</span> derives these perspectives from section headings of Wikipedia articles.</p>
<p>In SECTOR <span class="citation" data-cites="arnold2019sector">[@arnold2019sector]</span>, a benchmark for text segmentation is derived from Wikipedia article text with section boundaries.</p>
<p>In Entity Aspect Linking, the goal is to refine an existing entity link in text to state which of several aspects of an entity is most relevant. Benchmarks were created from context of Wikipedia hyperlinks to a Wikipedia article’s section, where the article identifies the entity, and the section its most relevant aspect <span class="citation" data-cites="nanni2018entity ramsdell2020large">[@nanni2018entity; @ramsdell2020large]</span>.</p>
<p>In multi-hop question answering relevant information across multiple documents need to be combined to answer the question. <span class="citation" data-cites="welbl2018constructing">@welbl2018constructing</span> construct the Quangaroo dataset from a chain of relation triples on Wikidata. For each entity on relation chains, the lead texts of entity’s articles are provided.</p>
<p>Previously, slot-filling and relation extraction approaches have been trained to extract information from Wikipedia infoboxes or Wikidata from the accompanying article <span class="citation" data-cites="kasneci2009yago hewlett2016wikireading">[@kasneci2009yago; @hewlett2016wikireading]</span>.</p>
<h2 id="related-resource-releases">Related Resource Releases</h2>
<p>As part of the TREC Complex Answer Retrieval (CAR) track<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a>, we provided a <em>Wikimark</em> based on a dump of the English Wikipedia from 2016. However, to avoid cheating in the TREC CAR track, the software and toolchain to produce the <em>Wikimark</em> was not released (but we release it now).</p>
<p>Since the dump provided by Wikimedia is difficult to parse, we provided an easily-machine readable dump for English Wikipedia from 2018 and 2020 as a service to TREC News and TREC Conversational Assistance tracks.</p>
<p>In 2020, we provided a Wikimark for Entity Aspect Linking based in the 2020 English Wikipedia dump.</p>
<h2 id="what-is-new-about-this-resource">What is New about <em>this</em> Resource?</h2>
<p>In contrast to previous releases, this resource provides software and customizable toolchain for converting Wikipedia dumps and harvesting <em>Wikimarks</em>. We are providing Wikipedia dumps for recent dumps from January 1st 2022. In addition to previously offered English Wikipedia dumps, we also provide conversions from Simple English and Japanese Wikipedias. We provide <em>Wikimarks</em> for query-specific clustering and entity linking, alongside previously available benchmarks for retrieval. Moreover, all datasets are now available as gzipped JSONL formats, although previously used CBOR formats are offered as well.</p>
<h1 id="resource-creation-approach">Resource Creation Approach</h1>
<p>We create the <em>Wikimark</em> resource by first converting the Wikipedia dump to an easily machine-readable format, then processing it, extracting subsets from which <em>Wikimarks</em> are derived for a range of tasks.</p>
<h2 id="sec:unprocessed">Wikipedia Dump Conversion</h2>
<p>Mediawiki offers “XML dumps” of the raw Wikitext markup,<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a> as it was edited by authors of Wikipedia articles. A major challenge is the abundance of markup mistakes (due to the manual editing). The Wikitext markup is designed to be rendered by an error tolerant template engine that iteratively replaces markup with rendered text. There is no clear specification of how the markup is to be interpreted in the light of errors.<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a></p>
<p>A common source of errors arise from user mistakes in parenthetical markup expressions, such as links, templates, bold and italics markup or quotes which have missing end markers. Such mistakes renders Wikitext a very difficult format to parse. We use a PEG parser-generator that is able to efficiently backtrack and recover when content contains ill-formed expressions according to the grammar. However, we abandoned attempts of parsing italics and bold formatting, as these contain the highest numbers of markup errors (and hence parsing ambiguities).</p>
<p>Mediawiki supports a large variety of inline templates, which are used for formatting, tagging, common patterns, and temporal expressions. For example the templates <code>when</code> and <code>as-of</code> keep time expressions up to date. While by default, our processing pipeline deletes template expressions, our the language specific configuration file allows to provide static substitutions to avoid incomplete sentences.</p>
<p>Since we developed this parser in 2016, several parallel initiatives provided better access to Wikipedia data, such as MediaWiki’s Parsoid<a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a>. While most of these libraries are targeting HTML and clear-text generation, our focus is on preserving all the semantic information associated with different elements of a Wikipedia article. In particular, we provide the following information about Wikipedia articles, as depicted in Figure <a href="#fig:example" data-reference-type="ref" data-reference="fig:example">1</a>:</p>
<figure>
<embed src="horseshoe-crab-wikipedia.pdf" id="fig:example" /><figcaption aria-hidden="true">Example of provided information from a Wikipedia article. License: CC BY-SA 3.0; Redacted for brevity.<span id="fig:example" label="fig:example">[fig:example]</span></figcaption>
</figure>
<dl>
<dt>Title:</dt>
<dd><p>Used as page name and entity name. Page IDs are derived as URL-encoding of the page name. Example “Horseshoe crab”.</p>
</dd>
<dt>Wikidata QID:</dt>
<dd><p>The Wikidata IDs (<code>Qxxx</code>) is exposed as page metadata along with the wiki site identifier (e.g. “enwiki”). This allows cross-referencing corresponding pages across different dumps and languages. Example “<code>Q1329239</code>”</p>
</dd>
<dt>Paragraphs:</dt>
<dd><p>Each paragraph is represented as chunks of plain text and links. The paragraph ID is derived from an MD5 hash of the visible text.</p>
</dd>
<dt>Links:</dt>
<dd><p>Each internal hyperlink is preserved with anchor text, target page, and if applicable, target section. Example “brackish” links to the page “Brackish water”.</p>
</dd>
<dt>Lead Text:</dt>
<dd><p>Used as a short description of the entity, it is preserved as one or more paragraphs. Example “Horseshoe crabs are marine...”</p>
</dd>
<dt>Sections and Headings:</dt>
<dd><p>Sections are preserved with content and headings. Sections can contain subsections recursively. Heading IDs are derived as URL-encoding of the heading text. Example: “Threats” (top-level section)</p>
</dd>
<dt>Administrative Sections:</dt>
<dd><p>Some sections include meta information about a page rather than actual content, often such sections are available for all Wikipedia pages, examples are “References”, “Further reading”, or “External links”. Our pipeline will first convert them as-is but supports the removal of those sections if desired.</p>
</dd>
<dt>Lists:</dt>
<dd><p>Bulleted lists are preserved with indentation level. The content is represented as a paragraph.</p>
</dd>
<dt>Images:</dt>
<dd><p>Images are preserved with URL to the graphic and their caption, which is represented as a paragraph. Example “Underside of two horseshoe crabs...”</p>
</dd>
<dt>Infobox:</dt>
<dd><p>Infoboxes are preserved with their name (Example “biota”) and represented as a list of key, value pairs. Keys are strings, while values can be any element above (paragraph, image, etc).</p>
</dd>
<dt>Categories:</dt>
<dd><p>The category information is preserved in the page metadata (Example “Xiphosura”)</p>
</dd>
<dt>Redirects:</dt>
<dd><p>When a page is renamed, the old page name presents a redirect to the new page. This is a useful resource of alternative names. We post-process the dump to expose this information in the page metadata. Example the plural form “Horseshoe crabs”.</p>
</dd>
<dt>In-Links:</dt>
<dd><p>While outlinks of a page can be derived from the link information of a page, we post-process all pages to collect hyperlinks linking to any given page. This information is preserved in the page metadata (both as page IDs and page names).</p>
</dd>
<dt>Disambiguations:</dt>
<dd><p>Disambiguation pages that refer to this page as exposed in the page metadata. Disambiguation pages provide information about pages that share an ambiguous name. The Wikitext denotes this information as a disambiguation template. Unfortunately, different languages can use language-specific disambiguation templates for example in German “Begriffsklärung”. These have to be customized in the language-specific pipeline configuration.</p>
</dd>
<dt>Page tags:</dt>
<dd><p>Some interesting page information is preserved as page-level templates, examples are “Vital articles” or “Good articles” which are identified by a committee. The tags are configurable and will be exposed as page metadata.</p>
</dd>
</dl>
<p>The results of this conversion are available in the <code>unprocessedAll</code> package. It contains article pages, category pages, and disambiguation pages. This package is most useful to analyze conversion errors or templates that need to be customized in the language-dependent configuration.</p>
<p>Researchers who prefer to build their own <em>Wikimark</em> processing pipeline, are recommended to base their code on this data set.</p>
<h2 id="sec:processed">Processing and Deduplication</h2>
<p>The following pipeline steps will filter and deduplicate the dump before splitting it into <em>Wikimarks</em>.</p>
<h4 id="remove-pages.">Remove pages.</h4>
<p>In the next step, we discard category, disambiguation, and list pages since most of their information is preserved in the metadata of article pages. The exact filter predicate can be configured by adjusting the <code>config.filterPredicates</code> property.</p>
<h4 id="remove-sections.">Remove sections.</h4>
<p>Then the content of each article is transformed by removing infoboxes, the category information at bottom of the page, and administrative headings. Page metadata is preserved. The filtering options can be configured with <code>config.pageProcessing</code> using options listed in <code>config.pageProcessing</code>.</p>
<h4 id="deduplication-optional.">Deduplication (optional).</h4>
<p>There is an abundance of duplicated content on Wikipedia. Since the paragraph ID is based on a hash of the visible content, this provides an easy solution for removing identical content. However, there still remain many near-duplicates. For English pages, we provide a deduplication mechanism: All paragraphs are divided into buckets with a GloVE-based locality-sensitive hash. Next, every pair of paragraphs within the same bucket that has 95% bigram overlap is called a duplicate. After repeating the randomized process five times and applying the transitivity of duplicate relationships, we derive clusters of near-duplicate paragraphs. For each cluster, one representative paragraph is chosen.</p>
<p>Next all remaining articles in the collection are re-written to replace paragraphs in the duplicate cluster with its cluster representative. All following pipeline steps apply to this collection.</p>
<h4 id="paragraph-corpussecparagraph-corpus.">Paragraph Corpus<span id="sec:paragraph-corpus" label="sec:paragraph-corpus">[sec:paragraph-corpus]</span>.</h4>
<p>All paragraphs are extracted from of all articles, shuffled, and provided as a paragraph corpus.</p>
<h4 id="article-cleaning.">Article Cleaning.</h4>
<p>To ensure that only high-quality text-centric articles are chosen during <em>Wikimark</em> creation, we remove sections with very short headings (less than three letters) or very long headings (more than 100 characters) as these indicate pages with mal-formed Wikitext. We remove images and their captions as well as sections without textual content. We also remove any page with less than three remaining sections, as these indicate pages based on visual content.</p>
<h4 id="wikimarks."><em>Wikimarks</em>.</h4>
<p>We recommend to derive <em>Wikimarks</em> from these cleaned articles. The processing pipeline will associate each article with a train vs test split and one of five folds. To obtain random, yet reproducible results, we use a deterministic hash of the page name to assign a page to one of these splits, which are consistent across all following steps.</p>
<p>Our pipeline allows the selection of subsets of articles via a wide range of predicates described in Table <a href="#tab:predicate" data-reference-type="ref" data-reference="tab:predicate">[tab:predicate]</a>. For example, subsets can be selected om the basis of category membership, existence of a page tag, or when explicitly listed via page title or Wikidata QID. Several predicates can be combined with simple logic expressions.</p>
<h1 id="automatic-benchmarks-aka-wikimarks">Automatic Benchmarks aka <em>Wikimarks</em></h1>
<p><em>Wikimarks</em> are created from a subset of pages, such as lists of Wikidata QIDs or category memberships. The page subset is separated into a test set and five train folds. For each of them task-specific datasets, such as queries, candidate sets, and relevance ground truth’s for the <em>Wikimarks</em> are exported. By default the following information is provided for each dataset:<a href="#fn9" class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a></p>
<dl>
<dt>Articles <span class="math inline">†</span>:</dt>
<dd><p>Content of processed articles (JSONL or CBOR).</p>
</dd>
<dt>Titles/QIDs:</dt>
<dd><p>Page titles and Wikidata QIDs of pages in this subset.</p>
</dd>
<dt>Paragraphs <span class="math inline">†</span>:</dt>
<dd><p>Corpus of paragraphs from this article subset.</p>
</dd>
<dt>Provenance:</dt>
<dd><p>Information about the Wikipedia dump the subset originated from.</p>
</dd>
</dl>
<p>Additionally, task-specific <em>Wikimark</em> data is provides as described in the following.</p>
<figure>
<embed src="rel-cluster-benchmark.pdf" /><figcaption aria-hidden="true"><em>Wikimarks</em> derived for article-level retrieval and clustering (left) from a given article (right). Paragraph IDs indicated by numbers in black dots; entity IDs as letters in stick figures; ground truth cluster indexes.</figcaption>
</figure>
<h2 id="retrieval-wikimark">Retrieval <em>Wikimark</em></h2>
<p>The retrieval <em>Wikimark</em> is designed to study the quality of retrieval models. For queries derived from Wikipedia titles, any paragraph originating from the Wikipedia article is counted as relevant. This <em>Wikimark</em> was referred to as the “automatic ground truth” in the TREC Complex Answer Retrieval task.</p>
<p><em>Wikimarks</em> for three kinds of retrieval scenarios are provided:</p>
<ul>
<li><p>Article: The query is the page title, and the goal is to retrieve paragraphs that are relevant for this query. For the passage retrieval relevance data (i.e., qrels) any paragraph located anywhere on the original page is counted as relevant, all other paragraphs are non-relevant.</p></li>
<li><p>Toplevel: The query is a combination of page title and heading of a top-level section. The goal is to retrieve paragraphs that are in fact located within this section or one of its subsections.</p></li>
<li><p>Hierarchical: The query is derived from any section on the page. The goal is to retrieve paragraphs that are exactly in this section, not a subsection.</p></li>
</ul>
<p>In addition to passage-level retrieval, we also provide a <em>Wikimark</em> for entity retrieval, where any entity (as represented by their Wikipedia pages) that is linked to from a relevant paragraph is regarded as relevant.</p>
<p>As a corpus for retrieving passages from, we recommend to use the paragraph corpus described in Section <a href="#sec:paragraph-corpus" data-reference-type="ref" data-reference="sec:paragraph-corpus">[sec:paragraph-corpus]</a>. As a legal set of entities, we recommend to use an unprocessed dump of Wikipedia pages.</p>
<p>For the retrieval <em>Wikimark</em>, we provide the following information:</p>
<dl>
<dt>Outlines:</dt>
<dd><p>Title and section outlines of the articles, to derive query texts from. Page metadata is available.</p>
</dd>
<dt>Topics:</dt>
<dd><p>Query IDs for each section—these can also be obtained from the outlines.</p>
</dd>
<dt>Passage Qrels <span class="math inline">†</span>:</dt>
<dd><p><code>Trec-eval</code> compatible qrels files of paragraph IDs for article-level retrieval, top-level section retrieval, and hierarchical section retrieval.</p>
</dd>
<dt>Entity Qrels <span class="math inline">†</span>:</dt>
<dd><p><code>Trec-eval</code> compatible qrels files of entity IDs (same as page IDs) for article, top-level section, and hierarchical section retrieval.</p>
</dd>
</dl>
<h4 id="evaluation.">Evaluation.</h4>
<p>We recommend to use the retrieval evaluation tool <code>trec-eval</code><a href="#fn10" class="footnote-ref" id="fnref10" role="doc-noteref"><sup>10</sup></a> with option <code>-c</code> using the provided qrels files.</p>
<figure>
<embed src="entity-linking-benchmark.pdf" /><figcaption aria-hidden="true"><em>Wikimarks</em> derived for query-specific entity linking (bottom) from a the second paragraph (top). The task is to annotate the plain text with entity links (for example with entities a, d, and e). True entities d and e are derived from hyperlinks contained in this paragraph (bold) with given character spans. Since entity a was linked in a previous paragraph and its annotation is to be accepted without penalty.</figcaption>
</figure>
<h2 id="sec:wikimark">Query-specific Clustering <em>Wikimark</em></h2>
<p>The task of search result clustering, will, given a search query and a ranking of search results, identify query-specific clusters for presentation. We provide a <em>Wikimark</em> dataset for this clustering task, where the query is taken as page title, and each top-level section defines one ground truth cluster. The search results are taken from the article-level retrieval task. To train on this task in isolation from a retrieval system, we suggest to use all passages that originate from this page.</p>
<p>The query-specific clustering <em>Wikimark</em> is provided as a JSONL gzipped file<a href="#fn11" class="footnote-ref" id="fnref11" role="doc-noteref"><sup>11</sup></a> which contains the following information:</p>
<dl>
<dt>Query:</dt>
<dd><p>The query text is derived from the page name; the query ID from the page ID.</p>
</dd>
<dt>Elements:</dt>
<dd><p>List of paragraph IDs contained on the page.</p>
</dd>
<dt>True Cluster Labels <span class="math inline">†</span>:</dt>
<dd><p>List of true cluster labels for each element. The <span class="math inline"><em>i</em></span>’th cluster label is derived from the section ID of the top-level section where the <span class="math inline"><em>i</em></span>’th element is located.</p>
</dd>
<dt>True Cluster Index <span class="math inline">†</span>:</dt>
<dd><p>Projecting the true cluster labels onto integers from <span class="math inline">0, 1…</span>.</p>
</dd>
</dl>
<p>In this <em>Wikimark</em>, we remove instances with less than two clusters.</p>
<h4 id="evaluation">Evaluation</h4>
<p>Our <em>Wikimark</em> format is designed to work with <code>scikit.learn</code>’s cluster evaluation measures.<a href="#fn12" class="footnote-ref" id="fnref12" role="doc-noteref"><sup>12</sup></a> This evaluation assumes that true clusters are represented as indexes <span class="math inline">0, 1, …</span>. The evaluation measure uses a list of true labels, were for the <code>element</code> at position <span class="math inline"><em>i</em></span>, the true cluster index is located at position <span class="math inline"><em>i</em></span> of the list <code>true cluster label</code>. To evaluate a predicted clustering, represent each cluster on its own scale from <span class="math inline">0, 1, …</span> (different from true labels), an produce a list of predicted cluster labels in the order of given elements <span class="math inline"><em>i</em></span>. We recommend to evaluate using the Adjusted RAND index evaluation metric, which ranges from -1 (worst) to +1 (best) with 0 referring to a random clustering.</p>
<div id="tab:data-stats">
<table>
<caption>Number of articles in each <em>Wikimark</em> subset.<span id="tab:data-stats" label="tab:data-stats">[tab:data-stats]</span></caption>
<thead>
<tr class="header">
<th></th>
<th style="text-align: right;">en</th>
<th style="text-align: right;">simple</th>
<th style="text-align: right;">ja</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>vital-articles.test</td>
<td style="text-align: right;">521</td>
<td style="text-align: right;">461</td>
<td style="text-align: right;">503</td>
</tr>
<tr class="even">
<td>vital-articles.train</td>
<td style="text-align: right;">528</td>
<td style="text-align: right;">471</td>
<td style="text-align: right;">539</td>
</tr>
<tr class="odd">
<td>good-articles.test</td>
<td style="text-align: right;">17,086</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">809</td>
</tr>
<tr class="even">
<td>good-articles.train</td>
<td style="text-align: right;">17,361</td>
<td style="text-align: right;">2</td>
<td style="text-align: right;">838</td>
</tr>
<tr class="odd">
<td>US-history.test</td>
<td style="text-align: right;">4,232</td>
<td style="text-align: right;">9</td>
<td style="text-align: right;">--</td>
</tr>
<tr class="even">
<td>US-history.train</td>
<td style="text-align: right;">4,284</td>
<td style="text-align: right;">13</td>
<td style="text-align: right;">--</td>
</tr>
<tr class="odd">
<td>horseshoe-crab.train</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">--</td>
</tr>
<tr class="even">
<td>benchmarkY1.test</td>
<td style="text-align: right;">131</td>
<td style="text-align: right;">44</td>
<td style="text-align: right;">71</td>
</tr>
<tr class="odd">
<td>benchmarkY1.train</td>
<td style="text-align: right;">117</td>
<td style="text-align: right;">42</td>
<td style="text-align: right;">81</td>
</tr>
<tr class="even">
<td>car-train-large.train</td>
<td style="text-align: right;">884,709</td>
<td style="text-align: right;">17,335</td>
<td style="text-align: right;">246,649</td>
</tr>
<tr class="odd">
<td>test200.test</td>
<td style="text-align: right;">--</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">42</td>
</tr>
<tr class="even">
<td>test200.train</td>
<td style="text-align: right;">188</td>
<td style="text-align: right;">12</td>
<td style="text-align: right;">44</td>
</tr>
</tbody>
</table>
</div>
<h2 id="sec:entity-link-wikimark">Query-specific Entity Linking <em>Wikimark</em></h2>
<p>Entity linking is typically discussed as an NLP task that ignores the context of a search query. However when presenting relevant information for a search query, maybe it would be best not to annotate all possible entity links, but instead focus on linking entities that are relevant for the query. <em>Wikimarks</em> allow us to create a query-specific entity linking dataset, as Wikipedia’s editorial policies are to only include hyperlink to pages when the information is relevant for the topic of the article.</p>
<p>The query-specific entity linking <em>Wikimark</em> is provided as a JSONL gzipped file which contains the following information:</p>
<dl>
<dt>Query:</dt>
<dd><p>The query text is derived from the page name; the query ID from the page ID.</p>
</dd>
<dt>Text-only Paragraph:</dt>
<dd><p>The text contents of paragraph (without entities links), to be annotated with entity links.</p>
</dd>
<dt>True Linked Paragraph <span class="math inline">†</span>:</dt>
<dd><p>The original paragraph (with links) for training and as ground truth.</p>
</dd>
<dt>True Entity Labels <span class="math inline">†</span>:</dt>
<dd><p>List of entity IDs that should be linked in this paragraph. These are provided as internal PageIDs as well as Wikidata QIDs.</p>
</dd>
<dt>Acceptable Entity Labels <span class="math inline">†</span>:</dt>
<dd><p>List of acceptable entity IDs that can be linked in this paragraph without penalty. List of entities linked in this paragraph and any previous paragraph. These are provided as internal PageIDs as well as Wikidata QIDs.</p>
</dd>
</dl>
<p>We remove instances of paragraphs without any linked entities.</p>
<p>Wikipedia’s editorial policies mandate that entities are only linked once per article. Consequently, entities that are mentioned repeatedly are only linked once. Since the entity linking ground truth is derived from hyperlinks, entity linking predictions would get penalized for linking all these entities. To alleviate this without resorting to heuristics, we collect all entities linked in all preceding paragraphs of an article and exposed them as <code>acceptable entity labels</code>. The entity linking evaluation should only give credit to every entity in <code>true labels</code>, but not penalize entities in <code>acceptable labels</code>.</p>
<h4 id="evaluation.-1">Evaluation.</h4>
<p>Unfortunately, there are no established evaluation frameworks for entity linking. We recommend to separately evaluate the set of correctly linked entities and the offsets of annotated spans as macro-average on mean-squared error of character offsets. A widely used evaluation measure is the F1 measure on the predicted entity set (referred to as <code>predicted labels</code>). For the F1 measure we suggest to derive statistics from the size of label sets as follows.</p>
<dl>
<dt>TP:</dt>
<dd><p><code>predicted labels</code> <span class="math inline">∩</span> <code>true labels</code></p>
</dd>
<dt>FP:</dt>
<dd><p><code>predicted labels</code> <span class="math inline">\</span> <code>acceptable labels</code></p>
</dd>
<dt>FN:</dt>
<dd><p><code>true labels</code> <span class="math inline">\</span> <code>predicted labels</code></p>
</dd>
</dl>
<p>We suggest to evaluate the quality of annotated text spans via the set of all true labels <span class="math inline"><em>T</em></span>. The per-entity span error is the root mean squared error on predicted spans <span class="math inline">(<em>b</em><sub><em>p</em></sub>,<em>e</em><sub><em>p</em></sub>)</span> versus spans <span class="math inline">(<em>b</em><sub><em>t</em></sub>,<em>e</em><sub><em>t</em></sub>)</span>.</p>
<h4 id="interfacing-with-entity-linking-software.">Interfacing with Entity Linking Software.</h4>
<p>A wide range of entity linking software is available, such as the TagMe-successor “WAT”<a href="#fn13" class="footnote-ref" id="fnref13" role="doc-noteref"><sup>13</sup></a> or Rel <span class="citation" data-cites="van2020rel">[@van2020rel]</span>. A practical issue lies in using unique identifiers of entities in linking systems. While more and more entity linking systems are using stable Wikidata QIDs, today, many entity linking systems still use English Wikipedia page titles to uniquely identify entities—however pages are occasionally renamed, and page titles on Simple English might be different.</p>
<p>While the Wikidata API provides a lookup-service, it is not practical for large-scale entity linking experiments. To support this situation, we provide a lookup resource for converting Wikipedia page titles to Wikidata QIDs, which includes previously renamed titles.</p>
<h1 id="provided-dumps-and-wikimarks">Provided Dumps and <em>Wikimarks</em></h1>
<p>Since running our conversion pipeline can take several days, we demonstrate its usefulness by providing three Wikipedia dumps from January 1st, 2022 for English Wikipedia, Simple English Wikipedia, and Japanese Wikipedia.</p>
<p>For each Wikipedia, we provide separately both for CBOR and JSONL output formats:</p>
<dl>
<dt><code>unprocessedAll</code>:</dt>
<dd><p>an unprocessed dump of all Wikipedia articles, as described in Section <a href="#sec:unprocessed" data-reference-type="ref" data-reference="sec:unprocessed">3.1</a>.</p>
</dd>
<dt><code>collection</code>:</dt>
<dd><p>all resources needed to perform experiments:</p>
<dl>
<dt><code>benchmarks</code>:</dt>
<dd><p><em>Wikimarks</em> for different subsets, as detailed below.</p>
</dd>
<dt><code>paragraphCorpus</code>:</dt>
<dd><p>a corpus for passage retrieval as described in Section <a href="#sec:processed" data-reference-type="ref" data-reference="sec:processed">3.2</a>.</p>
</dd>
<dt><code>unprocessedAllButBenchmark</code>:</dt>
<dd><p>an unprocessed Wikipedia dump in lieu of a knowledge graph (pages from which <em>Wikimarks</em> were created are held out).</p>
</dd>
</dl>
</dd>
</dl>
<p>We provide an example <em>Wikimarks</em> to demonstrate the versatility of the pipeline. These subsets are highly configurable and can be extended in the Wikipedia-specific configuration. Table <a href="#tab:data-stats" data-reference-type="ref" data-reference="tab:data-stats">1</a> lists the number of pages contained in each train/test set of the subsets.</p>
<dl>
<dt>name-contains SUBSTR</dt>
<dd><p>matches pages where the page name contains the SUBSTR (case insensitive)</p>
</dd>
<dt>name-has-prefix PREFIX</dt>
<dd><p>matches pages where the page name starts with PREFIX (case sensitive)</p>
</dd>
<dt>name-has-suffix SUFFIX</dt>
<dd><p>matches pages where the page name ends with SUFFIX (case sensitive)</p>
</dd>
<dt>category-contains SUBSTR</dt>
<dd><p>matches pages that are a member of a category that contains SUBSTR (case insensitive)</p>
</dd>
<dt>name-in-set ["P1", "P2", "P3"</dt>
<dd><p>] matches pages whose page names are in the given set P1,P2,P3</p>
</dd>
<dt>name-or-redirect-in-set ["P1", ..</dt>
<dd><p>] same as name-in-set but also matches in redirects (useful when pages are renamed)</p>
</dd>
<dt>pageid-in-set ["P1", "P2", "P3"</dt>
<dd><p>] matches pages whose page IDs are in the given set P1,P2,P3</p>
</dd>
<dt>qid-in-set ["Q1", "Q2", "Q3"</dt>
<dd><p>] matches pages with Wikidata QIDs in the given set Q1,Q2,Q3 (note requires pages file with populated QIDs)</p>
</dd>
<dt>has-page-tag ["T1", ...</dt>
<dd><p>] matches pages with the given Page Tags, e.g. "Good article"</p>
</dd>
<dt>true</dt>
<dd><p>always true</p>
</dd>
<dt>PRED1 <span class="math inline">|</span> PRED2</dt>
<dd><p>Boolean OR, matches predicate PRED1 or PRED2</p>
</dd>
<dt>PRED1 &amp; PRED2</dt>
<dd><p>Boolean AND, matches predicate PRED1 and PRED2</p>
</dd>
<dt>! PRED</dt>
<dd><p>Boolean NOT, inverts the predicate PRED</p>
</dd>
<dt>page-hash-mod N K [SALT</dt>
<dd><p>] matches pages where (hash of the page name) <span class="math inline"> mod  <em>N</em> =  = <em>K</em></span>, for <span class="math inline"><em>N</em> &gt; <em>K</em></span></p>
</dd>
</dl>

<dl>
<dt>Vital-articles:</dt>
<dd><p>A set of important articles that the Wikipedia community identified<a href="#fn14" class="footnote-ref" id="fnref14" role="doc-noteref"><sup>14</sup></a>. The community strives to provide these articles for all languages. We obtain the set of Vital articles via Wikidata, then filter the processed articles by Wikidata QID.<br />
Predicate <code>qid-set-from-file "./vital-articles.qids"</code></p>
</dd>
<dt>Good-articles:</dt>
<dd><p>A Wikipedia committee defines a set of good articles<a href="#fn15" class="footnote-ref" id="fnref15" role="doc-noteref"><sup>15</sup></a> that are well-written, contain factually accurate and verifiable information and are of broad importance. Such pages are identified either as template “GA” or “good article”, which our pipeline is configured to expose as page tag “Good article”.<br />
Predicate: <code>has-page-tag ["Good article"]</code></p>
</dd>
<dt>US-history:</dt>
<dd><p>A set of pages in categories that contain the words “United” “States” “history”, such as “History of the United States” or “United States history timelines”.<br />
Predicate: <code>(category-contains "history" &amp; category-contains "united" &amp; category-contains "states")</code></p>
</dd>
<dt>Horseshoe-crab:</dt>
<dd><p>The single Wikipedia page on horseshoe crabs used in the example above. It is identified by its Wikidata QID.<br />
Predicate: <code>qid-in-set ["Q1329239"]</code></p>
</dd>
</dl>
<p>For backwards compatibility, we also provide subsets used in the TREC Complex Answer Retrieval track.</p>
<dl>
<dt>BenchmarkY1:</dt>
<dd><p>Train/Test set used in the first year of TREC CAR Articles manually selected by browsing then randomly split into train/test.</p>
</dd>
<dt>Car-train-large:</dt>
<dd><p>Intended to be a very large training set for CAR, excludes pages categories related to people, events, works of art, sports clubs and lists. This set was originally known in CAR as “train-v2.0” and used to filter the training set. Note, that the general training set produced by our pipeline includes any category.</p>
</dd>
<dt>Test200:</dt>
<dd><p>An initial test of two hundred training pages selected from a list of articles in training fold 0. Some of these articles were later identified to be of weak quality (and removed from Wikipedia) and this subset was not officially used.</p>
</dd>
</dl>
<p>Since <code>car-train-large</code> and <code>test200</code> subsets do not provide a test set, all files in the test directory are empty.</p>
<h1 id="experimental-reference-baselines">Experimental Reference Baselines</h1>
<p>We are providing reference baselines for our example <em>Wikimarks</em> derived for the benchmarkY1 subsets from English and Simple English Wikipedia. The total number of test instances for each <em>Wikimark</em> are given in Table <a href="#tab:table-num-instances" data-reference-type="ref" data-reference="tab:table-num-instances">[tab:table-num-instances]</a>. The results obtained with our recommended evaluation approaches are presented in Table <a href="#tab:results" data-reference-type="ref" data-reference="tab:results">[tab:results]</a>.</p>
<div class="table*">
<div class="footnotesize">
<table>
<tbody>
<tr class="odd">
<td></td>
<td style="text-align: right;">Relevant Passages</td>
<td></td>
<td></td>
<td></td>
<td style="text-align: right;">Relevant Entities</td>
<td></td>
<td></td>
<td></td>
<td style="text-align: right;">Clustering Instances</td>
<td></td>
<td></td>
<td></td>
<td style="text-align: right;">Entity Linking Instances</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td></td>
<td style="text-align: right;">en</td>
<td>simple</td>
<td>ja</td>
<td></td>
<td style="text-align: right;">en</td>
<td>simple</td>
<td>ja</td>
<td></td>
<td style="text-align: right;">en</td>
<td>simple</td>
<td>ja</td>
<td></td>
<td style="text-align: right;">en</td>
<td>simple</td>
<td>jp</td>
<td></td>
</tr>
<tr class="odd">
<td>vital-articles.test</td>
<td style="text-align: right;">44,444</td>
<td>7,117</td>
<td>12,448</td>
<td></td>
<td style="text-align: right;">159,392</td>
<td>20,626</td>
<td>38,975</td>
<td></td>
<td style="text-align: right;">521</td>
<td>328</td>
<td>393</td>
<td></td>
<td style="text-align: right;">64,857</td>
<td>9,339</td>
<td>23,217</td>
<td></td>
</tr>
<tr class="even">
<td>vital-articles.train</td>
<td style="text-align: right;">42,008</td>
<td>6,845</td>
<td>13,330</td>
<td></td>
<td style="text-align: right;">149,609</td>
<td>19,401</td>
<td>42,357</td>
<td></td>
<td style="text-align: right;">528</td>
<td>324</td>
<td>440</td>
<td></td>
<td style="text-align: right;">61,984</td>
<td>8,663</td>
<td>25,743</td>
<td></td>
</tr>
<tr class="odd">
<td>good-articles.test</td>
<td style="text-align: right;">408,454</td>
<td>7</td>
<td>23,869</td>
<td></td>
<td style="text-align: right;">1429,087</td>
<td>47</td>
<td>65,031</td>
<td></td>
<td style="text-align: right;">17,088</td>
<td>1</td>
<td>626</td>
<td></td>
<td style="text-align: right;">777,081</td>
<td>8</td>
<td>27,903</td>
<td></td>
</tr>
<tr class="even">
<td>good-articles.train</td>
<td style="text-align: right;">415,034</td>
<td>17</td>
<td>24,375</td>
<td></td>
<td style="text-align: right;">1465,327</td>
<td>87</td>
<td>61,050</td>
<td></td>
<td style="text-align: right;">17,362</td>
<td>1</td>
<td>626</td>
<td></td>
<td style="text-align: right;">789,726</td>
<td>39</td>
<td>27,538</td>
<td></td>
</tr>
<tr class="odd">
<td>US-history.test</td>
<td style="text-align: right;">83,213</td>
<td>176</td>
<td>–</td>
<td></td>
<td style="text-align: right;">206,672</td>
<td>405</td>
<td>–</td>
<td></td>
<td style="text-align: right;">4,232</td>
<td>6</td>
<td>–</td>
<td></td>
<td style="text-align: right;">169,014</td>
<td>210</td>
<td>–</td>
<td></td>
</tr>
<tr class="even">
<td>US-history.train</td>
<td style="text-align: right;">83,255</td>
<td>146</td>
<td>–</td>
<td></td>
<td style="text-align: right;">205,438</td>
<td>608</td>
<td>–</td>
<td></td>
<td style="text-align: right;">4,285</td>
<td>7</td>
<td>–</td>
<td></td>
<td style="text-align: right;">160,764</td>
<td>173</td>
<td>–</td>
<td></td>
</tr>
<tr class="odd">
<td>horseshoe-crab.train</td>
<td style="text-align: right;">21</td>
<td>11</td>
<td>–</td>
<td></td>
<td style="text-align: right;">69</td>
<td>40</td>
<td>–</td>
<td></td>
<td style="text-align: right;">1</td>
<td>1</td>
<td>–</td>
<td></td>
<td style="text-align: right;">44</td>
<td>13</td>
<td>–</td>
<td></td>
</tr>
<tr class="even">
<td>benchmarkY1.test</td>
<td style="text-align: right;">6,554</td>
<td>434</td>
<td>1,160</td>
<td></td>
<td style="text-align: right;">15,698</td>
<td>1,117</td>
<td>3,018</td>
<td></td>
<td style="text-align: right;">131</td>
<td>23</td>
<td>56</td>
<td></td>
<td style="text-align: right;">8,536</td>
<td>454</td>
<td>1,978</td>
<td></td>
</tr>
<tr class="odd">
<td>benchmarkY1.train</td>
<td style="text-align: right;">5,588</td>
<td>449</td>
<td>1,396</td>
<td></td>
<td style="text-align: right;">14,744</td>
<td>1,273</td>
<td>3,440</td>
<td></td>
<td style="text-align: right;">117</td>
<td>25</td>
<td>60</td>
<td></td>
<td style="text-align: right;">7,258</td>
<td>513</td>
<td>2,152</td>
<td></td>
</tr>
<tr class="even">
<td>car-train-large.train</td>
<td style="text-align: right;">9,254,925</td>
<td>113,444</td>
<td>1496,289</td>
<td></td>
<td style="text-align: right;">19,764,159</td>
<td>249,369</td>
<td>3,462,123</td>
<td></td>
<td style="text-align: right;">885,014</td>
<td>6,918</td>
<td>87,012</td>
<td></td>
<td style="text-align: right;">25,423,934</td>
<td>185,203</td>
<td>3824,333</td>
<td></td>
</tr>
<tr class="odd">
<td>test200.train</td>
<td style="text-align: right;">5,537</td>
<td>109</td>
<td>335</td>
<td></td>
<td style="text-align: right;">12,345</td>
<td>272</td>
<td>929</td>
<td></td>
<td style="text-align: right;">188</td>
<td>5</td>
<td>19</td>
<td></td>
<td style="text-align: right;">9,147</td>
<td>135</td>
<td>612</td>
<td></td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="table*">
<table>
<tbody>
<tr class="odd">
<td></td>
<td></td>
<td style="text-align: right;">simple</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td style="text-align: right;">en</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td style="text-align: right;">benchmarkY1.train</td>
<td></td>
<td></td>
<td></td>
<td>benchmarkY1.test</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td style="text-align: right;">benchmarkY1.train</td>
<td></td>
<td></td>
<td></td>
<td>benchmarkY1.test</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td>Paragraph Retrieval [MAP]</td>
<td></td>
<td style="text-align: right;"></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td style="text-align: right;"></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td>bm25</td>
<td></td>
<td style="text-align: right;"><strong>0</strong></td>
<td><strong>31</strong></td>
<td><strong>0</strong></td>
<td><strong>04</strong></td>
<td><strong>0</strong></td>
<td><strong>29</strong></td>
<td><strong>0</strong></td>
<td><strong>03</strong></td>
<td></td>
<td></td>
<td style="text-align: right;">0</td>
<td>097</td>
<td>0</td>
<td>01</td>
<td>0</td>
<td>094</td>
<td>0</td>
<td>01</td>
</tr>
<tr class="odd">
<td>bm25-rm3</td>
<td></td>
<td style="text-align: right;">0</td>
<td>29</td>
<td>0</td>
<td>04</td>
<td>0</td>
<td>26</td>
<td>0</td>
<td>03</td>
<td></td>
<td></td>
<td style="text-align: right;"><strong>0</strong></td>
<td><strong>107</strong></td>
<td><strong>0</strong></td>
<td><strong>01</strong></td>
<td><strong>0</strong></td>
<td><strong>101</strong></td>
<td><strong>0</strong></td>
<td><strong>01</strong></td>
</tr>
<tr class="even">
<td>QL-rm3</td>
<td></td>
<td style="text-align: right;">0</td>
<td>25</td>
<td>0</td>
<td>04</td>
<td>0</td>
<td>20</td>
<td>0</td>
<td>02</td>
<td></td>
<td></td>
<td style="text-align: right;">0</td>
<td>084</td>
<td>0</td>
<td>01</td>
<td>0</td>
<td>076</td>
<td>0</td>
<td>01</td>
</tr>
<tr class="odd">
<td>Entity Ranking [MAP]</td>
<td></td>
<td style="text-align: right;"></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td style="text-align: right;"></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td>page-bm25</td>
<td></td>
<td style="text-align: right;">0</td>
<td>03</td>
<td>0</td>
<td>005</td>
<td>0</td>
<td>038</td>
<td>0</td>
<td>007</td>
<td></td>
<td></td>
<td style="text-align: right;">0</td>
<td>025</td>
<td>0</td>
<td>002</td>
<td>0</td>
<td>026</td>
<td>0</td>
<td>003</td>
</tr>
<tr class="odd">
<td>page-bm25-rm3</td>
<td></td>
<td style="text-align: right;">0</td>
<td>05</td>
<td>0</td>
<td>007</td>
<td>0</td>
<td>048</td>
<td>0</td>
<td>007</td>
<td></td>
<td></td>
<td style="text-align: right;">0</td>
<td>037</td>
<td>0</td>
<td>003</td>
<td>0</td>
<td>038</td>
<td>0</td>
<td>004</td>
</tr>
<tr class="even">
<td>paragraph-bm25-ECM</td>
<td></td>
<td style="text-align: right;"><strong>0</strong></td>
<td><strong>23</strong></td>
<td><strong>0</strong></td>
<td><strong>03</strong></td>
<td><strong>0</strong></td>
<td><strong>253</strong></td>
<td><strong>0</strong></td>
<td><strong>021</strong></td>
<td></td>
<td></td>
<td style="text-align: right;"><strong>0</strong></td>
<td><strong>215</strong></td>
<td><strong>0</strong></td>
<td><strong>01</strong></td>
<td><strong>0</strong></td>
<td><strong>21</strong></td>
<td><strong>0</strong></td>
<td><strong>01</strong></td>
</tr>
<tr class="odd">
<td>Cluster [Adjusted RAND Index]</td>
<td></td>
<td style="text-align: right;"></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td style="text-align: right;"></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td>TF-IDF agglomerative</td>
<td></td>
<td style="text-align: right;">0</td>
<td>16</td>
<td>0</td>
<td>06</td>
<td>0</td>
<td>27</td>
<td>0</td>
<td>07</td>
<td></td>
<td></td>
<td style="text-align: right;">0</td>
<td>15</td>
<td>0</td>
<td>01</td>
<td>0</td>
<td>16</td>
<td>0</td>
<td>01</td>
</tr>
<tr class="odd">
<td>TF-IDF kmeans</td>
<td></td>
<td style="text-align: right;">0</td>
<td>13</td>
<td>0</td>
<td>01</td>
<td>0</td>
<td>12</td>
<td>0</td>
<td>01</td>
<td></td>
<td></td>
<td style="text-align: right;">0</td>
<td>11</td>
<td>0</td>
<td>04</td>
<td><strong>0</strong></td>
<td><strong>19</strong></td>
<td><strong>0</strong></td>
<td><strong>05</strong></td>
</tr>
<tr class="even">
<td>SBERT kmeans</td>
<td></td>
<td style="text-align: right;"><strong>0</strong></td>
<td><strong>38</strong></td>
<td><strong>0</strong></td>
<td><strong>09</strong></td>
<td><strong>0</strong></td>
<td><strong>38</strong></td>
<td><strong>0</strong></td>
<td><strong>09</strong></td>
<td></td>
<td></td>
<td style="text-align: right;"><strong>0</strong></td>
<td><strong>23</strong></td>
<td><strong>0</strong></td>
<td><strong>02</strong></td>
<td><strong>0</strong></td>
<td><strong>19</strong></td>
<td><strong>0</strong></td>
<td><strong>01</strong></td>
</tr>
<tr class="odd">
<td>Entity Linking [Paragraph-Macro-avg F1]</td>
<td></td>
<td style="text-align: right;"></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td style="text-align: right;"></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td>WAT</td>
<td></td>
<td style="text-align: right;"><strong>0</strong></td>
<td><strong>44</strong></td>
<td><strong>0</strong></td>
<td><strong>01</strong></td>
<td><strong>0</strong></td>
<td><strong>42</strong></td>
<td><strong>0</strong></td>
<td><strong>01</strong></td>
<td></td>
<td></td>
<td style="text-align: right;"><strong>0</strong></td>
<td><strong>332</strong></td>
<td><strong>0</strong></td>
<td><strong>004</strong></td>
<td><strong>0</strong></td>
<td><strong>310</strong></td>
<td><strong>0</strong></td>
<td><strong>003</strong></td>
</tr>
</tbody>
</table>
</div>
<h2 id="passage-retrieval">Passage Retrieval</h2>
<p>We provide reference results for article-level retrieval, using page titles as queries. Paragraphs are retrieved from the <code>paragraphCorpus</code> of each Wikipedia dump respectively.</p>
<p>Baseline implementations are based on Lucene, with code provided online.<a href="#fn16" class="footnote-ref" id="fnref16" role="doc-noteref"><sup>16</sup></a> As baselines we include:</p>
<dl>
<dt>bm25:</dt>
<dd><p>Lucene’s BM25 method.</p>
</dd>
<dt>bm25-rm3:</dt>
<dd><p>RM3 query expansion, then retrieve with BM25.</p>
</dd>
<dt>QL-rm3:</dt>
<dd><p>RM3 query expansion, then retrieve with Lucene’s Dirichlet-smoothed query likelihood.</p>
</dd>
</dl>
<p>As can be seen in Table <a href="#tab:results" data-reference-type="ref" data-reference="tab:results">[tab:results]</a>, all methods perform reasonably in terms of MAP. For brevity we are omitting R-precision, reciprocal rank, and NDCG results, which lead to similar conclusions.</p>
<p>Note that in this study, article-level results are reported - this is unlike results for TREC CAR which are based on hierarchical-level retrieval.</p>
<h2 id="entity-retrieval">Entity Retrieval</h2>
<p>In article-level entity retrieval, page titles are used as queries. Entities are retrieved from the <code>allButBenchmark</code> articles of each respective Wikipedia dump.</p>
<p>Baseline implementations are based on Lucene using an index of pages in <code>allButBenchmark</code> and an index of paragraphs (with links) of the <code>paragraphCorpus</code>. As baselines we include:</p>
<dl>
<dt>page-bm25:</dt>
<dd><p>Retrieving Wikipedia pages via BM25.</p>
</dd>
<dt>page-bm25-rm3:</dt>
<dd><p>RM3 query expansion, then retrieving pages with BM25.</p>
</dd>
<dt>paragraph-bm25-ECM:</dt>
<dd><p>Retrieving paragraphs with BM25, then ranking entities linked in these paragraphs with the entity context model (ECM).</p>
</dd>
</dl>
<p>The entity context model (ECM) <span class="citation" data-cites="chatterjee2021entity">[@chatterjee2021entity]</span> represents documents in a feedback run as a language model over entity links, then uses an RM3-style expansion to derive a distribution over entity links. While <span class="citation" data-cites="dalton2014entity">@dalton2014entity</span> used these entities as expansion terms, here we use them as a means of ranking entities.</p>
<p>Table <a href="#tab:results" data-reference-type="ref" data-reference="tab:results">[tab:results]</a> demonstrates that retrieving entities via their Wikipedia pages obtains poor results, while ECM, which uses entity links in a feedback set obtains good results. This is in line with earlier findings <span class="citation" data-cites="chatterjee2021entity">[@chatterjee2021entity]</span>.</p>
<h2 id="clustering">Clustering</h2>
<p>We provide reference results for the following clustering methods based on TF-IDF and clustering in default configuration as provided by <code>scikit.learn</code>.<a href="#fn17" class="footnote-ref" id="fnref17" role="doc-noteref"><sup>17</sup></a> We assume knowledge of the true number of clusters.</p>
<dl>
<dt>TF-IDF agglomerative:</dt>
<dd><p>Each paragraph is represented as a TF-IDF vector, then using agglomerative clustering with Euclidean distance.</p>
</dd>
<dt>TF-IDF kmeans:</dt>
<dd><p>TF-IDF paragraph representation, then using K-means clustering.</p>
</dd>
<dt>SBERT kmeans:</dt>
<dd><p>Using Sentence-BERT paragraph representation (using ), then using K-means clustering.</p>
</dd>
</dl>
<p>Sentence-BERT <span class="citation" data-cites="reimers2019sentence">[@reimers2019sentence]</span> is a BERT-based embedding model trained for clustering sentences. We are using the <code>bert-base-uncased</code> version provided by the authors.</p>
<p>We evaluate the predicted clusterings using the Adjusted Rand Index. Results are given in Table <a href="#tab:results" data-reference-type="ref" data-reference="tab:results">[tab:results]</a>. We observe that Sentence-BERT performs best.</p>
<h2 id="entity-linking">Entity Linking</h2>
<p>We provide reference results for entity linking with the WAT entity linker<a href="#fn18" class="footnote-ref" id="fnref18" role="doc-noteref"><sup>18</sup></a> <span class="citation" data-cites="piccinno2014wat">[@piccinno2014wat]</span> using its default configuration.</p>
<p>Since the WAT linker used entity IDs based on Wikipedia titles of an unknown dump, we use our title to Wikidata QID conversion resource to annotate entity linking spans with QIDs. We follow the evaluation paradigm discussed in Section <a href="#sec:entity-link-wikimark" data-reference-type="ref" data-reference="sec:entity-link-wikimark">4.3</a>. Results are evaluated in F1 per paragraph, we provide macro-averages over paragraphs.</p>
<p>The results demonstrate that an external entity linking tool can obtain reasonable performance on our <em>Wikimark</em>. While results are omitted, we obtain similar performance for micro-averaged F1 (0.29), and query-based macro-averaged-F1 (0.30) for benchmarkY1.test set from English Wikipedia. We find 15,561 true positives, 70,120 false positives, and 4,965 false negatives.</p>
<h1 id="conclusions">Conclusions</h1>
<p>We provide a resource for generating <em>Wikimarks</em>—automatically harvested benchmarks from Wikipedia. We provide an efficient implementation for deriving <em>Wikimarks</em> from any Wikipedia dump. Our pipeline handles the download of Wikipedia files, the dump conversion, processing, subset creation, and derivation of four example <em>Wikimarks</em> for passage and entity retrieval, query-specific clustering, and query-specific entity linking. The pipeline is configurable to adjust for different page subsets, different languages, and is extensible towards additional <em>Wikimarks</em>. While the software was originally developed to provide data for the TREC Complex Answer Retrieval track, it has been significantly extended towards multiple languages and additional <em>Wikimarks</em>, Wikidata integration new JSONL output formats.</p>
<p>As a proof of concept, we provide three recent Wikipedia dumps for English, Simple English, and Japanese from January 1st, 2022, along with <em>Wikimarks</em> for different page subsets. We provide results of reference baseline results for retrieval, clustering, and entity linking on these datasets, which are in-line with previous findings.</p>
<p>Other communities have successfully leveraged content from Wikipedias for benchmark creation, and studies on TREC CAR demonstrated that findings under automatic <em>Wikimarks</em> are in line with results based on manual annotations of trained assessors. Nevertheless, <em>Wikimarks</em> are not a replacement for manually annotated benchmarks. However, we hope to contribute a means for providing evaluations for early research developments and training of data-hungry machine learning methods. This approach can overcome challenges for novel information retrieval tasks, such as search result organization and result annotation for information-seeking.</p>
<h1 class="unnumbered" id="acknowledgements">Acknowledgements</h1>
<p>This material is based upon work supported by the National Science Foundation under Grant No. 1846017. Disclaimer. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation.</p>
<p>We would like to thank Hideo Joho for providing the Japanese language-configuration for this pipeline. We thank all participants TREC CAR for being early adopters, and anyone who engaged with us in discussion on how to derive additional benchmarks from Wikipedia.</p>
<section class="footnotes" role="doc-endnotes">
<hr />
<ol>
<li id="fn1" role="doc-endnote"><p><a href="https://www.wikidata.org" class="uri">https://www.wikidata.org</a><a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2" role="doc-endnote"><p><a href="https://jsonlines.org/" class="uri">https://jsonlines.org/</a><a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3" role="doc-endnote"><p><a href="https://www.rfc-editor.org/rfc/rfc8949.html" class="uri">https://www.rfc-editor.org/rfc/rfc8949.html</a><a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4" role="doc-endnote"><p><a href="https://fever.ai/dataset/feverous.html" class="uri">https://fever.ai/dataset/feverous.html</a><a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5" role="doc-endnote"><p><a href="https://trec-car.cs.unh.edu" class="uri">https://trec-car.cs.unh.edu</a><a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6" role="doc-endnote"><p><a href="https://en.wikipedia.org/wiki/Help:Wikitext" class="uri">https://en.wikipedia.org/wiki/Help:Wikitext</a><a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn7" role="doc-endnote"><p><a href="https://www.mediawiki.org/wiki/Markup_spec" class="uri">https://www.mediawiki.org/wiki/Markup_spec</a><a href="#fnref7" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn8" role="doc-endnote"><p><a href="https://www.mediawiki.org/wiki/Parsoid" class="uri">https://www.mediawiki.org/wiki/Parsoid</a><a href="#fnref8" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn9" role="doc-endnote"><p>Data marked with <span class="math inline">†</span> is reserved for training and evaluation only.<a href="#fnref9" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn10" role="doc-endnote"><p>https://github.com/usnistgov/trec_eval<a href="#fnref10" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn11" role="doc-endnote"><p>Because of the redundancy induced by repeated JSON keys, we recommend to open the file as a Gzipped stream, rather than decompressing it.<a href="#fnref11" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn12" role="doc-endnote"><p><a href="https://scikit-learn.org/stable/modules/clustering.html" class="uri">https://scikit-learn.org/stable/modules/clustering.html</a><a href="#fnref12" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn13" role="doc-endnote"><p><a href="https://sobigdata.d4science.org/web/tagme/wat-api" class="uri">https://sobigdata.d4science.org/web/tagme/wat-api</a><a href="#fnref13" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn14" role="doc-endnote"><p>Vital articles <a href="https://en.wikipedia.org/wiki/Wikipedia:Vital_articles" class="uri">https://en.wikipedia.org/wiki/Wikipedia:Vital_articles</a><a href="#fnref14" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn15" role="doc-endnote"><p>Good articles <a href="https://en.wikipedia.org/wiki/Wikipedia:Good_articles" class="uri">https://en.wikipedia.org/wiki/Wikipedia:Good_articles</a><a href="#fnref15" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn16" role="doc-endnote"><p>Passage and Entity Retrieval: <a href="https://github.com/laura-dietz/trec-car-methods" class="uri">https://github.com/laura-dietz/trec-car-methods</a><a href="#fnref16" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn17" role="doc-endnote"><p><code>sklearn.feature_extraction.text</code> and <code>sklearn.cluster</code> in version 1.0.2<a href="#fnref17" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn18" role="doc-endnote"><p><a href="https://sobigdata.d4science.org/web/tagme/wat-api" class="uri">https://sobigdata.d4science.org/web/tagme/wat-api</a><a href="#fnref18" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>
        </section>
      </div>
    </div>
  </body>
</html>

