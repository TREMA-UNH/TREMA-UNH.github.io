<!DOCTYPE html>
<html>
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
    <title>TREMA @ UNH - index.old</title>
    <link rel="stylesheet" type="text/css" href="../css/default.css" />
    <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  </head>
  <body>
    <div id="container">
      <div id="content">
        <div id="sidebar">
          <nav>
            <h1>Navigation</h1>
            <ul>
              <li><a href="../">Home</a></li>
              <li><a href="../wikimarks">Wikimarks</a></li>
            </ul>
          </nav>
        </div>

        <section id="main">
        <h1 id="wikimarks">Wikimarks</h1>
<p>We provide a methodology and tool-set for harvesting relevance benchmarks for a variety of tasks from Wikipedia. We call these benchmarks <em>Wikimarks</em>. This work is an extension of the infrastructure developed while organizing the <a href="http://trec-car.cs.unh.edu/">Complex Answer Retrieval track</a> at <a href="https://trec.nist.gov/">NIST TREC</a> and examines several using Wikipedia to assess several tasks not previously considered in the TREC-CAR context.</p>
<p>We believe that Wikimarks can serve to complement traditional information retrieval benchmarks as they build upon a readily-available source of real-world text content. Furthermore, Wikipedia articles feature exhibit considerable machine-readable structure in the form of page structure, hyperlink structure, and complementary data sources such as <a href="https://wikidata.org/">Wikidata</a>.</p>
<h3 id="authors">Authors</h3>
<ul>
<li>Laura Dietz <a href="mailto:dietz@cs.unh.edu" class="email">dietz@cs.unh.edu</a></li>
<li>Shubham Chatterjee <a href="mailto:sc1242@wildcats.unh.edu" class="email">sc1242@wildcats.unh.edu</a></li>
<li>Connor Lennox <a href="mailto:cjl1053@wildcats.unh.edu" class="email">cjl1053@wildcats.unh.edu</a>,</li>
<li>Sumanta Kashyapi <a href="mailto:sk1105@wildcats.unh.edu" class="email">sk1105@wildcats.unh.edu</a>,</li>
<li>Pooja Oza <a href="mailto:pho1003@wildcats.unh.edu" class="email">pho1003@wildcats.unh.edu</a>,</li>
<li>Ben Gamari <a href="mailto:ben@well-typed.com" class="email">ben@well-typed.com</a></li>
</ul>
<p>University of New Hampshire, USA and Well-Typed LLP, UK</p>
<h2 id="table-of-contents">Table of Contents</h2>
<p>In this online supplement, we provide:</p>
<ul>
<li>An approach for deriving <a href="#wikimarks">wikimark collections</a>, i.e., Wikipedia-derived benchmarks,</li>
<li>for a set of four common information retrieval tasks</li>
<li>A <a href="#tools">set of tools</a> for deriving Wikimarks from Wikipedia dumps</li>
<li>A <a href="#conversions">set of machine-readable conversions</a> of the English, Simple English, and Japanese Wikipedias</li>
<li>A <a href="#collections">set of Wikimarks</a> derived from these three Wikipedias</li>
<li>An <a href="#evaluation">evaluation</a> of serveral baseline methods using these benchmarks</li>
</ul>
<h2 id="tools">Tools, Installation, and Usage</h2>
<p>Our pipeline for generating Wikimarks is found in the <a href="https://github.com/TREMA-UNH/trec-car-release"><code>TREMA-UNH/trec-car-release</code></a> project. Please follow installation, configuration and usage instructions in the <code>README</code>.</p>
<p>This pipeline builds upon the conversion tools provided by the <a href="https://github.com/TREMA-UNH/trec-car-create"><code>trec-car-create</code></a> package, which provides utilities for converting, extracting, inspecting, filtering, and generating benchmarks from Wikipedia. Please follow installation and compilation instructions described in the <code>README</code>.</p>
<p>Language bindings for java and python to read the CBOR file formats are provided in the <a href="https://github.com/TREMA-UNH/trec-car-tools"><code>trec-car-tools</code></a> packages.</p>
<h2 id="conversions">Wikipedia Conversions</h2>
<p>Along with the methodology and tools, we provide the raw products of our article conversion pipeline run on the English, Simple English, and Japanese Wikipedia dumps from 1 January 2022. These datasets, which we call <code>unprocessedAll</code>, include all pages of each Wiki in machine-readable JSONL or CBOR formats.</p>
<ul>
<li><p>English <code>en</code>: <a href="http://trec-car.cs.unh.edu/wikimarks/datareleases/wikimarks-20220101/wiki2022-en-collectionJsonl.tar">JSONL</a> / <a href="http://trec-car.cs.unh.edu/wikimarks/datareleases/wikimarks-20220101/wiki2022-en-collectionCbor.tar.xz">CBOR</a></p></li>
<li><p>Simple English <code>simple</code>: <a href="http://trec-car.cs.unh.edu/wikimarks/datareleases/wikimarks-20220101/wiki2022-simple-collectionJsonl.tar">JSONL</a> / <a href="http://trec-car.cs.unh.edu/wikimarks/datareleases/wikimarks-20220101/wiki2022-simple-collectionCbor.tar.xz">CBOR</a></p></li>
<li><p>Japanese <code>ja</code>: <a href="http://trec-car.cs.unh.edu/wikimarks/datareleases/wikimarks-20220101/wiki2022-ja-collectionJsonl.tar">JSONL</a> / <a href="http://trec-car.cs.unh.edu/wikimarks/datareleases/wikimarks-20220101/wiki2022-ja-collectionCbor.tar.xz">CBOR</a></p></li>
</ul>
<h3 id="data-model">Data Model</h3>
<p>Wikipedia articles, paragraphs, and outlines are provided in CBOR or JSONL are all represented with following this grammar. Wikipedia-internal hyperlinks are preserved through <code>ParaLink</code>s.</p>
<pre><code>     Page         -&gt; $pageName $pageId [PageSkeleton] PageType PageMetadata
     PageType     -&gt; ArticlePage | CategoryPage | RedirectPage ParaLink | DisambiguationPage
     PageMetadata -&gt; RedirectNames DisambiguationNames DisambiguationIds CategoryNames CategoryIds InlinkIds InlinkAnchors WikiDataQid SiteId PageTags
     RedirectNames       -&gt; [$pageName] 
     DisambiguationNames -&gt; [$pageName] 
     DisambiguationIds   -&gt; [$pageId] 
     CategoryNames       -&gt; [$pageName] 
     CategoryIds         -&gt; [$pageId] 
     InlinkIds           -&gt; [$pageId] 
     InlinkAnchors       -&gt; [$anchorText] 
     WikiDataQid         -&gt; [$qid] 
     SiteId              -&gt; [$siteId] 
     PageTags            -&gt; [$pageTags] 
     
     PageSkeleton -&gt; Section | Para | Image | ListItem | Infobox
     Section      -&gt; $sectionHeading [PageSkeleton]
     Para         -&gt; Paragraph
     Paragraph    -&gt; $paragraphId, [ParaBody]
     ListItem     -&gt; $nestingLevel, Paragraph
     Image        -&gt; $imageURL [PageSkeleton]
     ParaBody     -&gt; ParaText | ParaLink
     ParaText     -&gt; $text
     ParaLink     -&gt; $targetPage $targetPageId $targetPageQid $linkSection $anchorText
     Infobox      -&gt; $infoboxName [($key, [PageSkeleton])]</code></pre>
<h2 id="collections">Wikimark Collections</h2>
<p>In addition to the raw conversions described above, we also provide several Wikimarks extracted from these conversions:</p>
<ul>
<li><p>the <code>benchmarks</code> dataset provides Wikimarks for passage retrieval, entity retrieval, query-specific clustering, and entity linking, extracted from the page subsets described below.</p></li>
<li><p>the <code>unprocessedAllButBenchmark</code> dataset provides all pages except for the set included in <code>benchmarks</code> and is intended to be used for training of systems to be evaluated using <code>benchmarks</code>.</p></li>
<li><p>the <code>paragraphCorpus</code> dataset is a corpus of paragraphs from articles to be used for passage retrieval evaluation.</p></li>
</ul>
<p>These datasets are provided in JSONL or CBOR formats.</p>
<ul>
<li><p>English <code>en</code>: <a href="http://trec-car.cs.unh.edu/wikimarks/datareleases/wikimarks-20220101/wiki2022-en-collectionJsonl.tar">JSONL</a> / <a href="http://trec-car.cs.unh.edu/wikimarks/datareleases/wikimarks-20220101/wiki2022-en-collectionCbor.tar.xz">CBOR</a></p></li>
<li><p>Simple English <code>simple</code>: <a href="http://trec-car.cs.unh.edu/wikimarks/datareleases/wikimarks-20220101/wiki2022-simple-collectionJsonl.tar">JSONL</a> / <a href="http://trec-car.cs.unh.edu/wikimarks/datareleases/wikimarks-20220101/wiki2022-simple-collectionCbor.tar.xz">CBOR</a></p></li>
<li><p>Japanese <code>ja</code>: <a href="http://trec-car.cs.unh.edu/wikimarks/datareleases/wikimarks-20220101/wiki2022-ja-collectionJsonl.tar">JSONL</a> / <a href="http://trec-car.cs.unh.edu/wikimarks/datareleases/wikimarks-20220101/wiki2022-ja-collectionCbor.tar.xz">CBOR</a></p></li>
</ul>
<h3 id="page-subsets">Page Subsets</h3>
<p>The benchmarks described above are constructed from the following subset of Wikipedia pages from each of the provided Wikipedias:</p>
<dl>
<dt>Vital-articles:</dt>
<dd><p>A set of important articles that the Wikipedia community <a href="https://en.wikipedia.org/wiki/Wikipedia:Vital_articles">identified</a>. The community strives to provide these articles for all languages. We obtain the set of “vital” articles via Wikidata, then filter the processed articles by Wikidata QID.</p>
<p><code>trec-car-filter</code> Predicate: <code>qid-set-from-file "./vital-articles.qids"</code></p>
</dd>
<dt>Good-articles:</dt>
<dd><p>A Wikipedia committee defines a set of <a href="https://en.wikipedia.org/wiki/Wikipedia:Good_articles">good articles</a> that are well-written, contain factually accurate and verifiable information and are of broad importance. Such pages are identified either as template “GA” or “good article”, which our pipeline is configured to expose as page tag “Good article”.</p>
<p><code>trec-car-filter</code> Predicate: <code>has-page-tag ["Good article"]</code></p>
</dd>
<dt>US-history:</dt>
<dd><p>A set of pages in categories that contain the words “United” “States” “history”, such as “History of the United States” or “United States history timelines”.</p>
<p><code>trec-car-filter</code> Predicate: <code>(category-contains "history" &amp; category-contains "united" &amp; category-contains "states")</code></p>
</dd>
<dt>Horseshoe-crab:</dt>
<dd><p>The single Wikipedia page on horseshoe crabs used in the example above. It is identified by its Wikidata QID.</p>
<p><code>trec-car-filter</code> Predicate: <code>qid-in-set ["Q1329239"]</code></p>
</dd>
</dl>
<p>Additionally, we provide subsets used in the <a href="http://trec-car.cs.unh.edu/">TREC Complex Answer Retrieval Track</a> for backwards compatibility.</p>
<h3 id="subset-statistics">Subset Statistics</h3>
<div id="tab:data-stats">
<table>
<caption>Number of articles in each <em>Wikimark</em> subset.</caption>
<thead>
<tr class="header">
<th></th>
<th style="text-align: right;">en</th>
<th style="text-align: right;">simple</th>
<th style="text-align: right;">ja</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>vital-articles.test</td>
<td style="text-align: right;">521</td>
<td style="text-align: right;">461</td>
<td style="text-align: right;">503</td>
</tr>
<tr class="even">
<td>vital-articles.train</td>
<td style="text-align: right;">528</td>
<td style="text-align: right;">471</td>
<td style="text-align: right;">539</td>
</tr>
<tr class="odd">
<td>good-articles.test</td>
<td style="text-align: right;">17,086</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">809</td>
</tr>
<tr class="even">
<td>good-articles.train</td>
<td style="text-align: right;">17,361</td>
<td style="text-align: right;">2</td>
<td style="text-align: right;">838</td>
</tr>
<tr class="odd">
<td>US-history.test</td>
<td style="text-align: right;">4,232</td>
<td style="text-align: right;">9</td>
<td style="text-align: right;">--</td>
</tr>
<tr class="even">
<td>US-history.train</td>
<td style="text-align: right;">4,284</td>
<td style="text-align: right;">13</td>
<td style="text-align: right;">--</td>
</tr>
<tr class="odd">
<td>horseshoe-crab.train</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">--</td>
</tr>
<tr class="even">
<td>benchmarkY1.test</td>
<td style="text-align: right;">131</td>
<td style="text-align: right;">44</td>
<td style="text-align: right;">71</td>
</tr>
<tr class="odd">
<td>benchmarkY1.train</td>
<td style="text-align: right;">117</td>
<td style="text-align: right;">42</td>
<td style="text-align: right;">81</td>
</tr>
<tr class="even">
<td>car-train-large.train</td>
<td style="text-align: right;">884,709</td>
<td style="text-align: right;">17,335</td>
<td style="text-align: right;">246,649</td>
</tr>
<tr class="odd">
<td>test200.test</td>
<td style="text-align: right;">--</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">42</td>
</tr>
<tr class="even">
<td>test200.train</td>
<td style="text-align: right;">188</td>
<td style="text-align: right;">12</td>
<td style="text-align: right;">44</td>
</tr>
</tbody>
</table>
</div>
<h2 id="wikimarks">Wikimarks</h2>
<p>We provide a methodology for deriving <em>Wikimarks</em> for four common information retrieval tasks:</p>
<ul>
<li><em>passage retrieval</em>: retrieval of relevant text passages for a keyword query</li>
<li><em>entity retrieval</em>: retrieval of relevant <em>entities</em> (defined to be Wikipedia pages) for a keyword query</li>
<li><em>query-specific clustering</em>: sub-topic clustering of passages for a keyword query</li>
<li><em>query-specific entity-linking</em>: annotation of query-relevant entity links in relevant passages</li>
</ul>
<p><em>Wikimarks</em> are created from a subset of Wikipedia pages, such as lists of Wikidata QIDs or category memberships. The page subset is separated into a test set and five train folds. For each of them task-specific datasets, such as queries, candidate sets, and relevance ground truth’s for the <em>Wikimarks</em> are exported. By default the following information is provided for each dataset:</p>
<dl>
<dt>Articles <span class="math inline">†</span>:</dt>
<dd><p>Content of processed articles (JSONL or CBOR).</p>
</dd>
<dt>Titles/QIDs:</dt>
<dd><p>Page titles and Wikidata QIDs of pages in this subset.</p>
</dd>
<dt>Paragraphs <span class="math inline">†</span>:</dt>
<dd><p>Corpus of paragraphs from this article subset.</p>
</dd>
<dt>Provenance:</dt>
<dd><p>Information about the Wikipedia dump the subset originated from.</p>
</dd>
</dl>
<p>Additionally, task-specific <em>Wikimark</em> data is provides as described in the following.</p>
<figure>
<img src="rel-cluster-benchmark.png" alt="Wikimarks derived for article-level retrieval and clustering (left) from a given article (right). Paragraph IDs indicated by numbers in black dots; entity IDs as letters in stick figures; ground truth cluster indexes." /><figcaption aria-hidden="true"><em>Wikimarks</em> derived for article-level retrieval and clustering (left) from a given article (right). Paragraph IDs indicated by numbers in black dots; entity IDs as letters in stick figures; ground truth cluster indexes.</figcaption>
</figure>
<h3 id="retrieval-wikimark">Retrieval <em>Wikimark</em></h3>
<p>The retrieval <em>Wikimark</em> is designed to study the quality of retrieval models. For queries derived from Wikipedia titles, any paragraph originating from the Wikipedia article is counted as relevant. This <em>Wikimark</em> was referred to as the “automatic ground truth” in the TREC Complex Answer Retrieval task.</p>
<p><em>Wikimarks</em> for three kinds of retrieval scenarios are provided:</p>
<ul>
<li><p>Article: The query is the page title, and the goal is to retrieve paragraphs that are relevant for this query. For the passage retrieval relevance data (i.e., qrels) any paragraph located anywhere on the original page is counted as relevant, all other paragraphs are non-relevant.</p></li>
<li><p>Toplevel: The query is a combination of page title and heading of a top-level section. The goal is to retrieve paragraphs that are in fact located within this section or one of its subsections.</p></li>
<li><p>Hierarchical: The query is derived from any section on the page. The goal is to retrieve paragraphs that are exactly in this section, not a subsection.</p></li>
</ul>
<p>In addition to passage-level retrieval, we also provide a <em>Wikimark</em> for entity retrieval, where any entity (as represented by their Wikipedia pages) that is linked to from a relevant paragraph is regarded as relevant.</p>
<p>As a corpus for retrieving passages from, we recommend to use the paragraph corpus. As a legal set of entities, we recommend to use an unprocessed dump of Wikipedia pages.</p>
<p>For the retrieval <em>Wikimark</em>, we provide the following information:</p>
<dl>
<dt>Outlines:</dt>
<dd><p>Title and section outlines of the articles, to derive query texts from. Page metadata is available.</p>
</dd>
<dt>Topics:</dt>
<dd><p>Query IDs for each section—these can also be obtained from the outlines.</p>
</dd>
<dt>Passage Qrels <span class="math inline">†</span>:</dt>
<dd><p><code>Trec-eval</code> compatible qrels files of paragraph IDs for article-level retrieval, top-level section retrieval, and hierarchical section retrieval.</p>
</dd>
<dt>Entity Qrels <span class="math inline">†</span>:</dt>
<dd><p><code>Trec-eval</code> compatible qrels files of entity IDs (same as page IDs) for article, top-level section, and hierarchical section retrieval.</p>
</dd>
</dl>
<h4 id="evaluation.">Evaluation.</h4>
<p>We recommend to use the retrieval evaluation tool <a href="https://github.com/usnistgov/trec_eval"><code>trec-eval</code></a> with option <code>-c</code> using the provided qrels files.</p>
<h3 id="sec:wikimark">Query-specific Clustering <em>Wikimark</em></h3>
<p>The task of search result clustering, will, given a search query and a ranking of search results, identify query-specific clusters for presentation. We provide a <em>Wikimark</em> dataset for this clustering task, where the query is taken as page title, and each top-level section defines one ground truth cluster. The search results are taken from the article-level retrieval task. To train on this task in isolation from a retrieval system, we suggest to use all passages that originate from this page.</p>
<p>The query-specific clustering <em>Wikimark</em> is provided as a JSONL gzipped file which contains the following information:</p>
<dl>
<dt>Query:</dt>
<dd><p>The query text is derived from the page name; the query ID from the page ID.</p>
</dd>
<dt>Elements:</dt>
<dd><p>List of paragraph IDs contained on the page.</p>
</dd>
<dt>True Cluster Labels <span class="math inline">†</span>:</dt>
<dd><p>List of true cluster labels for each element. The <span class="math inline"><em>i</em></span>’th cluster label is derived from the section ID of the top-level section where the <span class="math inline"><em>i</em></span>’th element is located.</p>
</dd>
<dt>True Cluster Index <span class="math inline">†</span>:</dt>
<dd><p>Projecting the true cluster labels onto integers from <span class="math inline">0, 1…</span>.</p>
</dd>
</dl>
<p>In this <em>Wikimark</em>, we remove instances with less than two clusters.</p>
<figure>
<img src="entity-linking-benchmark.png" alt="Wikimarks derived for query-specific entity linking (bottom) from a the second paragraph (top). The task is to annotate the plain text with entity links (for example with entities a, d, and e). True entities d and e are derived from hyperlinks contained in this paragraph (bold) with given character spans. Since entity a was linked in a previous paragraph and its annotation is to be accepted without penalty." /><figcaption aria-hidden="true"><em>Wikimarks</em> derived for query-specific entity linking (bottom) from a the second paragraph (top). The task is to annotate the plain text with entity links (for example with entities a, d, and e). True entities d and e are derived from hyperlinks contained in this paragraph (bold) with given character spans. Since entity a was linked in a previous paragraph and its annotation is to be accepted without penalty.</figcaption>
</figure>
<h3 id="sec:entity-link-wikimark">Query-specific Entity Linking <em>Wikimark</em></h3>
<p>Entity linking is typically discussed as an NLP task that ignores the context of a search query. However when presenting relevant information for a search query, maybe it would be best not to annotate all possible entity links, but instead focus on linking entities that are relevant for the query. <em>Wikimarks</em> allow us to create a query-specific entity linking dataset, as Wikipedia’s editorial policies are to only include hyperlink to pages when the information is relevant for the topic of the article.</p>
<p>The query-specific entity linking <em>Wikimark</em> is provided as a JSONL gzipped file which contains the following information:</p>
<dl>
<dt>Query:</dt>
<dd><p>The query text is derived from the page name; the query ID from the page ID.</p>
</dd>
<dt>Text-only Paragraph:</dt>
<dd><p>The text contents of paragraph (without entities links), to be annotated with entity links.</p>
</dd>
<dt>True Linked Paragraph <span class="math inline">†</span>:</dt>
<dd><p>The original paragraph (with links) for training and as ground truth.</p>
</dd>
<dt>True Entity Labels <span class="math inline">†</span>:</dt>
<dd><p>List of entity IDs that should be linked in this paragraph. These are provided as internal PageIDs as well as Wikidata QIDs.</p>
</dd>
<dt>Acceptable Entity Labels <span class="math inline">†</span>:</dt>
<dd><p>List of acceptable entity IDs that can be linked in this paragraph without penalty. List of entities linked in this paragraph and any previous paragraph. These are provided as internal PageIDs as well as Wikidata QIDs.</p>
</dd>
</dl>
<p>We remove instances of paragraphs without any linked entities.</p>
<p>Wikipedia’s editorial policies mandate that entities are only linked once per article. Consequently, entities that are mentioned repeatedly are only linked once. Since the entity linking ground truth is derived from hyperlinks, entity linking predictions would get penalized for linking all these entities. To alleviate this without resorting to heuristics, we collect all entities linked in all preceding paragraphs of an article and exposed them as <code>acceptable entity labels</code>. The entity linking evaluation should only give credit to every entity in <code>true labels</code>, but not penalize entities in <code>acceptable labels</code>.</p>
<h4 id="wikimark-traintest-instances">Wikimark Train/Test Instances</h4>
<p>Number of train/test instances for each Wikimark.</p>
<div class="table*">
<div class="footnotesize">
<table>
<tbody>
<tr class="odd">
<td></td>
<td style="text-align: right;">Relevant Passages</td>
<td></td>
<td></td>
<td></td>
<td style="text-align: right;">Relevant Entities</td>
<td></td>
<td></td>
<td></td>
<td style="text-align: right;">Clustering Instances</td>
<td></td>
<td></td>
<td></td>
<td style="text-align: right;">Entity Linking Instances</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td></td>
<td style="text-align: right;">en</td>
<td>simple</td>
<td>ja</td>
<td></td>
<td style="text-align: right;">en</td>
<td>simple</td>
<td>ja</td>
<td></td>
<td style="text-align: right;">en</td>
<td>simple</td>
<td>ja</td>
<td></td>
<td style="text-align: right;">en</td>
<td>simple</td>
<td>jp</td>
<td></td>
</tr>
<tr class="odd">
<td>vital-articles.test</td>
<td style="text-align: right;">44,444</td>
<td>7,117</td>
<td>12,448</td>
<td></td>
<td style="text-align: right;">159,392</td>
<td>20,626</td>
<td>38,975</td>
<td></td>
<td style="text-align: right;">521</td>
<td>328</td>
<td>393</td>
<td></td>
<td style="text-align: right;">64,857</td>
<td>9,339</td>
<td>23,217</td>
<td></td>
</tr>
<tr class="even">
<td>vital-articles.train</td>
<td style="text-align: right;">42,008</td>
<td>6,845</td>
<td>13,330</td>
<td></td>
<td style="text-align: right;">149,609</td>
<td>19,401</td>
<td>42,357</td>
<td></td>
<td style="text-align: right;">528</td>
<td>324</td>
<td>440</td>
<td></td>
<td style="text-align: right;">61,984</td>
<td>8,663</td>
<td>25,743</td>
<td></td>
</tr>
<tr class="odd">
<td>good-articles.test</td>
<td style="text-align: right;">408,454</td>
<td>7</td>
<td>23,869</td>
<td></td>
<td style="text-align: right;">1429,087</td>
<td>47</td>
<td>65,031</td>
<td></td>
<td style="text-align: right;">17,088</td>
<td>1</td>
<td>626</td>
<td></td>
<td style="text-align: right;">777,081</td>
<td>8</td>
<td>27,903</td>
<td></td>
</tr>
<tr class="even">
<td>good-articles.train</td>
<td style="text-align: right;">415,034</td>
<td>17</td>
<td>24,375</td>
<td></td>
<td style="text-align: right;">1465,327</td>
<td>87</td>
<td>61,050</td>
<td></td>
<td style="text-align: right;">17,362</td>
<td>1</td>
<td>626</td>
<td></td>
<td style="text-align: right;">789,726</td>
<td>39</td>
<td>27,538</td>
<td></td>
</tr>
<tr class="odd">
<td>US-history.test</td>
<td style="text-align: right;">83,213</td>
<td>176</td>
<td>–</td>
<td></td>
<td style="text-align: right;">206,672</td>
<td>405</td>
<td>–</td>
<td></td>
<td style="text-align: right;">4,232</td>
<td>6</td>
<td>–</td>
<td></td>
<td style="text-align: right;">169,014</td>
<td>210</td>
<td>–</td>
<td></td>
</tr>
<tr class="even">
<td>US-history.train</td>
<td style="text-align: right;">83,255</td>
<td>146</td>
<td>–</td>
<td></td>
<td style="text-align: right;">205,438</td>
<td>608</td>
<td>–</td>
<td></td>
<td style="text-align: right;">4,285</td>
<td>7</td>
<td>–</td>
<td></td>
<td style="text-align: right;">160,764</td>
<td>173</td>
<td>–</td>
<td></td>
</tr>
<tr class="odd">
<td>horseshoe-crab.train</td>
<td style="text-align: right;">21</td>
<td>11</td>
<td>–</td>
<td></td>
<td style="text-align: right;">69</td>
<td>40</td>
<td>–</td>
<td></td>
<td style="text-align: right;">1</td>
<td>1</td>
<td>–</td>
<td></td>
<td style="text-align: right;">44</td>
<td>13</td>
<td>–</td>
<td></td>
</tr>
<tr class="even">
<td>benchmarkY1.test</td>
<td style="text-align: right;">6,554</td>
<td>434</td>
<td>1,160</td>
<td></td>
<td style="text-align: right;">15,698</td>
<td>1,117</td>
<td>3,018</td>
<td></td>
<td style="text-align: right;">131</td>
<td>23</td>
<td>56</td>
<td></td>
<td style="text-align: right;">8,536</td>
<td>454</td>
<td>1,978</td>
<td></td>
</tr>
<tr class="odd">
<td>benchmarkY1.train</td>
<td style="text-align: right;">5,588</td>
<td>449</td>
<td>1,396</td>
<td></td>
<td style="text-align: right;">14,744</td>
<td>1,273</td>
<td>3,440</td>
<td></td>
<td style="text-align: right;">117</td>
<td>25</td>
<td>60</td>
<td></td>
<td style="text-align: right;">7,258</td>
<td>513</td>
<td>2,152</td>
<td></td>
</tr>
<tr class="even">
<td>car-train-large.train</td>
<td style="text-align: right;">9,254,925</td>
<td>113,444</td>
<td>1496,289</td>
<td></td>
<td style="text-align: right;">19,764,159</td>
<td>249,369</td>
<td>3,462,123</td>
<td></td>
<td style="text-align: right;">885,014</td>
<td>6,918</td>
<td>87,012</td>
<td></td>
<td style="text-align: right;">25,423,934</td>
<td>185,203</td>
<td>3824,333</td>
<td></td>
</tr>
<tr class="odd">
<td>test200.train</td>
<td style="text-align: right;">5,537</td>
<td>109</td>
<td>335</td>
<td></td>
<td style="text-align: right;">12,345</td>
<td>272</td>
<td>929</td>
<td></td>
<td style="text-align: right;">188</td>
<td>5</td>
<td>19</td>
<td></td>
<td style="text-align: right;">9,147</td>
<td>135</td>
<td>612</td>
<td></td>
</tr>
</tbody>
</table>
</div>
</div>
<h2 id="evaluation">Evaluation Results for Reference Baselines</h2>
<h3 id="results">Results</h3>
<div class="table*">
<table>
<tbody>
<tr class="odd">
<td></td>
<td></td>
<td style="text-align: center;">simple</td>
<td></td>
<td></td>
<td style="text-align: center;">en</td>
<td></td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td style="text-align: center;">benchmarkY1.train</td>
<td>benchmarkY1.test</td>
<td></td>
<td style="text-align: center;">benchmarkY1.train</td>
<td>benchmarkY1.test</td>
</tr>
<tr class="odd">
<td>Paragraph Retrieval [MAP]</td>
<td></td>
<td style="text-align: center;"></td>
<td></td>
<td></td>
<td style="text-align: center;"></td>
<td></td>
</tr>
<tr class="even">
<td></td>
<td>bm25</td>
<td style="text-align: center;"><strong>0.31+/-0.04</strong></td>
<td><strong>0.29</strong>+/-<strong>0.03</strong></td>
<td></td>
<td style="text-align: center;">0.097+/-0.01</td>
<td>0.094+/-0.01</td>
</tr>
<tr class="odd">
<td></td>
<td>bm25-rm3</td>
<td style="text-align: center;">0.29+/-0.04</td>
<td>0.26+/-0.03</td>
<td></td>
<td style="text-align: center;"><strong>0.107</strong>+/-<strong>0.01</strong></td>
<td><strong>0.101</strong>+/-<strong>0.01</strong></td>
</tr>
<tr class="even">
<td></td>
<td>QL-rm3</td>
<td style="text-align: center;">0.25+/-0.04</td>
<td>0.20+/-0.02</td>
<td></td>
<td style="text-align: center;">0.084+/-0.01</td>
<td>0.076+/-0.01</td>
</tr>
<tr class="odd">
<td>Entity Ranking [MAP]</td>
<td></td>
<td style="text-align: center;"></td>
<td></td>
<td></td>
<td style="text-align: center;"></td>
<td></td>
</tr>
<tr class="even">
<td></td>
<td>page-bm25</td>
<td style="text-align: center;">0.03+/-0.005</td>
<td>0.038+/-0.007</td>
<td></td>
<td style="text-align: center;">0.025+/-0.002</td>
<td>0.026+/-0.003</td>
</tr>
<tr class="odd">
<td></td>
<td>page-bm25-rm3</td>
<td style="text-align: center;">0.05+/-0.007</td>
<td>0.048+/-0.007</td>
<td></td>
<td style="text-align: center;">0.037+/-0.003</td>
<td>0.038+/-0.004</td>
</tr>
<tr class="even">
<td></td>
<td>paragraph-bm25-ECM</td>
<td style="text-align: center;"><strong>0.23</strong>+/-<strong>0.03</strong></td>
<td><strong>0.253</strong>+/-<strong>0.021</strong></td>
<td></td>
<td style="text-align: center;"><strong>0.215</strong>+/-<strong>0.01</strong></td>
<td><strong>0.21</strong>+/-<strong>0.01</strong></td>
</tr>
<tr class="odd">
<td>Cluster [Adj. RAND]</td>
<td></td>
<td style="text-align: center;"></td>
<td></td>
<td></td>
<td style="text-align: center;"></td>
<td></td>
</tr>
<tr class="even">
<td></td>
<td>TF-IDF agglomerative</td>
<td style="text-align: center;">0.16+/-0.06</td>
<td>0.27+/-0.07</td>
<td></td>
<td style="text-align: center;">0.15+/-0.01</td>
<td>0.16+/-0.01</td>
</tr>
<tr class="odd">
<td></td>
<td>TF-IDF kmeans</td>
<td style="text-align: center;">0.13+/-0.01</td>
<td>0.12+/-0.01</td>
<td></td>
<td style="text-align: center;">0.11+/-0.04</td>
<td><strong>0.19</strong>+/-<strong>0.05</strong></td>
</tr>
<tr class="even">
<td></td>
<td>SBERT kmeans</td>
<td style="text-align: center;"><strong>0.38</strong>+/-<strong>0.09</strong></td>
<td><strong>0.38</strong>+/-<strong>0.09</strong></td>
<td></td>
<td style="text-align: center;"><strong>0.23</strong>+/-<strong>0.02</strong></td>
<td><strong>0.19</strong>+/-<strong>0.01</strong></td>
</tr>
<tr class="odd">
<td>Entity Linking [Paragraph-macro F1]</td>
<td></td>
<td style="text-align: center;"></td>
<td></td>
<td></td>
<td style="text-align: center;"></td>
<td></td>
</tr>
<tr class="even">
<td></td>
<td>WAT</td>
<td style="text-align: center;"><strong>0.44</strong>+/-<strong>0.01</strong></td>
<td><strong>0.42</strong>+/-<strong>0.01</strong></td>
<td></td>
<td style="text-align: center;"><strong>0.332</strong>+/-<strong>0.004</strong></td>
<td><strong>0.310</strong>+/-<strong>0.003</strong></td>
</tr>
</tbody>
</table>
</div>
<h3 id="baselines">Baselines</h3>
<h4 id="passage-and-entity-retrieval">Passage and Entity Retrieval</h4>
<p>Baseline implementations are based on Lucene, with code provided <a href="https://github.com/laura-dietz/trec-car-methods">online</a>.</p>
<p>Baselines for passage retrieval</p>
<dl>
<dt>bm25:</dt>
<dd><p>Lucene’s BM25 method.</p>
</dd>
<dt>bm25-rm3:</dt>
<dd><p>RM3 query expansion, then retrieve with BM25.</p>
</dd>
<dt>QL-rm3:</dt>
<dd><p>RM3 query expansion, then retrieve with Lucene’s Dirichlet-smoothed query likelihood.</p>
</dd>
</dl>
<p>Baselines for entity retrieval</p>
<dl>
<dt>page-bm25:</dt>
<dd><p>Retrieving Wikipedia pages via BM25.</p>
</dd>
<dt>page-bm25-rm3:</dt>
<dd><p>RM3 query expansion, then retrieving pages with BM25.</p>
</dd>
<dt>paragraph-bm25-ECM:</dt>
<dd><p>Retrieving paragraphs with BM25, then ranking entities linked in these paragraphs with the entity context model (ECM).</p>
</dd>
</dl>
<h4 id="clustering">Clustering</h4>
<p>Based on default implementations in <a href="https://scikit-learn.org/stable/modules/clustering.html#clustering-performance-evaluation"><code>scikit.learn</code></a> for TF-IDF, agglomerative clustering, and K-means clustering. We use packages <code>sklearn.feature_extraction.text</code> and <code>sklearn.cluster</code> in scikit.learn version 1.0.2</p>
<dl>
<dt>TF-IDF agglomerative:</dt>
<dd><p>Each paragraph is represented as a TF-IDF vector, then using agglomerative clustering with Euclidean distance.</p>
</dd>
<dt>TF-IDF kmeans:</dt>
<dd><p>TF-IDF paragraph representation, then using K-means clustering.</p>
</dd>
<dt>SBERT kmeans:</dt>
<dd><p>Using Sentence-BERT paragraph representation (using ), then using K-means clustering.</p>
</dd>
</dl>
<p>Sentence-BERT <span class="citation" data-cites="reimers2019sentence">[@reimers2019sentence]</span> is a BERT-based embedding model trained for clustering sentences. We are using the <code>bert-base-uncased</code> version provided by the authors.</p>
<h4 id="entity-linking">Entity Linking</h4>
<p>We provide reference results for entity linking with the <a href="https://sobigdata.d4science.org/web/tagme/wat-api">WAT entity linker</a> <span class="citation" data-cites="piccinno2014wat">[@piccinno2014wat]</span> using its default configuration.</p>
<h2 id="license">License</h2>
<p>This data set is part of the Wikimarks dataset version v2.6.</p>
<p>The conversions and benchmarks described above are provided by Laura Dietz, Ben Gamari under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/3.0/deed.en_US">Creative Commons Attribution-ShareAlike 3.0 Unported License</a>. The data is based on content extracted from <a href="https://dumps.wikipedia.org/" class="uri">https://dumps.wikipedia.org/</a> that is licensed under the Creative Commons Attribution-ShareAlike 3.0 Unported License.</p>
        </section>
      </div>
    </div>
  </body>
</html>

